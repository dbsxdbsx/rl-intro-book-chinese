# 第11章

# 离策略(Off-policy Methods)函数近似

​	相比于列表，采用函数近似的方法通过 boostrapping 进行离策略学习将会有很大的不同，并且被证明更困难。第6章和第7章中介绍的表格情况下 (tabular case) 的离策略方法可以很容易的扩展成半梯度法 (semi-gradient algorithms)，但是这些方法在离策略的条件下并不如同策略 (on-policy) 那样可以稳定收敛。在本章，我们通过深入线性函数近似的理论来进一步探讨这一收敛问题，并给出离策略条件下更容易保证收敛的算法。

​	回顾一下，在离策略条件下，给定策略$\mu$产生的数据，我们力求学习另一个策略$\pi$的值函数。在预测问题中，这两个策略$\mu$和$\pi$都是给定并且固定的，我们可以任意学习状态值$\hat{v}\approx v_\pi$或动作值$\hat{q}\approx q_\pi$。在控制问题中，我们学习的是动作值并且这两个策略在学习的过程中是持续变化的——策略$\pi$是关于动作值$\hat{q}$的贪婪策略，而策略$\mu$会更具有试探性，比如关于动作值$\hat{q}$的$\epsilon$-greedy策略。

​	离策略学习的关键主要可以分为两点：其中一个在表格情况和函数近似情况下都存在，另一个只存在于函数近似的情况。前一个与目标的学习更新有关，后一个和更新的数据分布有关。第6章和第7章中所介绍的关于重要性采样和抑制采样的技术手段可以解决前一个关键点，在表格情况下和函数近似情况下的所有的算法都需要有这样的技术。但是在函数近似的情况下，离策略更新的数据分布和同策略更新的数据分布并不一致，因此需要有更多的东西考虑。在策略更新的数据分布比较特殊，对于半梯度方法的稳定性有着至关重要的作用。通常有两大类方法可以解决这一更新分布的问题，其中一个是再一次使用重要性采样把需要更新的数据分布扭曲 (wrap) 成同策略的分布，这样半梯度的方法也足以保证收敛 (线性近似的情况下)。另一个解决方案是设计出利用真实的梯度进行更新的方法，从而算法的稳定性不依赖于任何特殊的数据分布。在这一章我们提出基于这两类方法的算法，这是增强学习中比较前沿的研究领域，在实际使用中很难说哪一类方法更加高效。

## 11.1 半梯度法(Semi-gradient Methods)

​	我们从早先的章节中学习到的离策略算法开始，来描述如何简单的扩展到函数近似情况下的半梯度算法。尽管这些方法可能会发散，看起来毫无意义，但是它们仍旧被经常使用。牢记这些方法在表格的情况下是可以保证收敛的，而表格的情况又是函数近似的一种特殊情况。所以在这样的条件下，将它们和一些特征选取方法相结合所组成的系统也是可以确保收敛的。无论怎样，这些方法足够简单，学习起来是一个不错的开始。

​	在第7章我们介绍了多种适用于离策略的算法。为了将它们转变成半梯度算法的形式，我们可以通过近似值函数 ($\hat{v}$ or $\hat{q}$) 和它的梯度把需要更新的变量 ($V$ or $Q$) 替换成权重向量 ($\theta$)。这其中的很多算法采用前一步 (pre-step) 的重要性采样比率：

$$\rho _t\doteq \frac{\pi(A _t|S _t)}{\mu(A _t|S _t)}$$ 

​	例如，一步更新的半梯度离策略 TD(0)算法除了$\rho _t$外，其它的部分都类似于对应的同策略算法：

$$\theta _{t+1}\doteq \theta _t + \alpha\rho _t\delta _t\nabla\hat{v}(S _t, \theta _t)$$ 

其中$\delta _t$根据所解决的问题是episode、discounted还是continuing、undiscounted有着不同的定义：

$$\delta _t\doteq R _{t+1} + \gamma\hat{v}(S _{t+1}, \theta _t) - \hat{v}(S _t, \theta _t)$$ 

$$\delta _t\doteq R _{t+1} - \overline R _t+ \hat{v}(S _{t+1}, \theta _t) - \hat{v}(S _t, \theta _t)$$ 

​	对于动作值，一步更新的算法被称作为半梯度Expected Sarsa (如果它的目标策略$\pi$是与当前动作值函数有关的贪婪策略，那么它相当于半梯度 Q-learning算法)：

$$\theta _{t+1}\doteq \theta _t + \alpha\delta _t\nabla\hat{q}(S _t, A _t, \theta _t)$$ 

$$\delta _t \doteq R _{t+1} + \gamma\sum _a\pi(a|S _{t+1})\hat{q}(S _{t+1}, a, \theta _t) - \hat{q}(S _t, A _t, \theta _t)$$  

$$\delta _t \doteq R _{t+1} - \overline R _t+ \sum _a\pi(a|S _{t+1})\hat{q}(S _{t+1}, a, \theta _t) - \hat{q}(S _t, A _t, \theta _t)$$ 

​	注意这个算法目前暂时没有使用重要性采样。在表格的情况下这么做很明显是合理的，但是在函数近似的情况下要不要使用重要性采样取决于你自己的判断。这一问题最终合适的解决方法有待于增强学习领域中函数近似理论的进一步完善和发展。

​	当把这些算法推广到多步更新的版本时，无论是状态值还是动作值的更新都需要考虑重要性采样比率。例如，对于$n$步更新的半梯度Expected Sarsa的更新公式有：

$$\theta _{t+n}\doteq\theta _{t+n-1} + \alpha\rho _{t+1}\cdot\cdot\cdot\rho _{t+n-1} [G _t^{(n)} - \hat{q}(S _t, A _t, \theta _{t+n-1})]\nabla\hat{q}(S _t, A _t, \theta _{t+n-1})$$  

$$G _t^{(n)}\doteq R _{t+1} + \cdot\cdot\cdot +\gamma^{n-1}R _{t+n} +\gamma^n\hat{q}(S _{t+n}, A _{t+n}, \theta _{t+n-1})$$

$$G _t^{(n)}\doteq R _{t+1} - \overline R _t +\cdot\cdot\cdot+R _{t+n} -\overline R _{t+n-1}+\hat{q}(S _{t+n}, A _{t+n}, \theta _{t+n-1})$$

*****

**复审的这块帮忙审一下, 后两个公式书上应该是印错了，期望sarsa实际的应该是** 


$$G _t^{(n)}\doteq R _{t+1} + \cdot\cdot\cdot +\gamma^{n-1}R _{t+n} +\gamma^n\sum _a\pi(a|S _{t+n})\hat{q}(S _{t+n}, a, \theta _{t+n-1})$$

$$G _t^{(n)}\doteq R _{t+1} - \overline R _t +\cdot\cdot\cdot+R _{t+n} -\overline R _{t+n-1}+\sum _a\pi(a|S _{t+n})\hat{q}(S _{t+n}, a, \theta _{t+n-1})$$

****

对于每个episode的结尾我们有一些不同的处理：在第一个公式中，当$t \ge T$时$\rho _t$取值为1，当$t +n \ge T$时$G _t^{(n)}$取值为$G _t$。

​	仔细回顾一下，在第7章我们提出了一种不需要重要性采样的离策略算法：$n$步 tree-backup算法。下面是它的半梯度更新公式：

$$\theta _{t+n} \doteq \theta _{t+n-1} + \alpha [G _t^{(n)} - \hat{q}(S _t, A _t, \theta _{t+n-1})]\nabla\hat{q}(S _t, A _t, \theta _{t+n-1})$$ 

$$G _t^{(n)} \doteq \hat{q}(S _t, A _t, \theta _{t-1}) + \sum _{k=t}^{t+n-1}\delta _k \prod _{i=t+1}^{k} \gamma\pi(A _i|S _i)$$ 

其中$\delta _t$和上面Expected Sarsa算法中的定义一样，当$t +n \ge T$时$G _t^{(n)} = G _t$，并且对于continuing的问题有$\gamma = 1$。在第7章我们还给出了所有计算动作值算法的统一形式：$n$-step $Q(\sigma)$。这里我们不再继续给出这个算法以及所有通过$n$步更新计算状态值算法的半梯度法公式，留给读者自行练习。

**Exercise 11.1**  将$n$步更新离策略 TD算法的公式(7.7)转换成半梯度法的形式，并给出相应的episode和continuing情况下的回报的定义。

***Exercise 11.2**  将$n$-step $Q(\sigma)$中的公式(7.9，7.13，7.14和7.15)转换成半梯度形式，并给出相应的episode和continuing情况下的各个变量的定义。



## 11.2 贝尔德(Baird)的反例##

​	在这一节，我们将要介绍一些目前已被周知并且很有启发意义的反例——在这些例子中半梯度法和其它的一些简单算法是不稳定并且发散的。

​	这其中最直观的例子是贝尔德反例(Baird's counterexample)。考虑图11.1所展示的具有两个动作、七个状态的episode问题。虚线所代表的动作将使得系统等概率进入上面留个状态之一，而实线所代表的动作将使得系统进入第七个状态。实际上，只有99%的概率会发生上述这些情况，还有剩下的1%概率会使得处于任意状态下的系统进入最终的状态，并结束当前的episode。(这意味着有一个99%的 discount rate。) 行为策略$\mu$采取两个动作的概率分别是6/7和1/7，所以下一个状态的分布并不是均匀的 (对于所以的非终止状态来说都是如此)，这也是这个episode的初始分布。目标策略$\pi$始终采取实线所代表的动作，所以在策略的状态分布将主要集中于第七个状态。所有的状态转移之间的奖励都为0。

![11-1](img\11-1.bmp)

​	图11.1: 贝尔德反例。对于这个马尔科夫过程的近似状态值函数如上图中每个状态内的线性表达式所示。采取实线的动作将进入第七个状态，而虚线的动作将会随机等概率的进入六个状态中的一个。整个episode在所有的状态转移过程中有1%的概率进入终止状态，即会有一个0.99的discount rate。所有的奖励都为0。



​	考虑对上图每个状态圆圈中所示的线性参数化的表达式的状态值进行估计。举例来说，对第一个状态的估计值是$2\theta _1+\theta _8$，其中下标代表了整个权重向量$\theta$的分量，也就是说第一个状态的特征向量为$\phi(1)=(2,0,0,0,0,0,0,1)^T$。由于状态转移过程中所有的奖励均为0，所以当$\theta=0$时，对于所有的状态$s$它们的真实状态值函数为$v _\pi(s)=0$。事实上，由于权重向量中元素的个数 (8) 多于非终止状态的个数 (7)，因此这一问题可以有很多解。此外，对应于状态函数的这一组特征向量$\{\phi(s): s\in\mathcal{S}\}$之间是线性独立的。从各个方面来说，它看起来似乎是线性函数近似的一个不错的例子。

​	如果我们采用半梯度 TD(0)算法来解决这一问题 (公式11.2)，那么权重会像图11.2所示一样发散到无穷大。无论我们把步长设置为多小，这一问题始终存在。事实上，即使是我们采用 DP (Dynamic Programming) 形式而不是学习的形式去进行backup的话，发散的问题也会发生。即在状态空间对权重向量$\theta _k$进行更新时，都采用如下 DP (full backup) 目标更新公式对每一个状态都执行同步的半梯度backup：

$$\theta _{k+1}\doteq\theta _k+\alpha\sum _s[E[R _{t+1}+\gamma\hat{v} _k(S _{t+1})|S _t=s]-\hat{v} _k(s)]\nabla\hat{v} _k(s)$$

在这一迭代更新条件下，不存在随机性和异步性。每个状态的更新都和经典 DP backup时一样。整个算法除了使用半梯度函数近似外，其它的部分都和经典算法相同。然而，这一系统仍然如图11.2所示一样是不稳定的。如果使用半梯度Q-learning算法的话 (公式11.3)，不稳定的情况仍旧会发生...

![11-2](img\11-2.bmp)

​	图11.2: 贝尔德反例的不稳定性的证明。步长尺寸是$\alpha=0.001$，初始权重为$\theta=(1,1,1,1,1,1,10,1)^T$。

​	

​	如果我们调整贝尔德反例中状态集的分布，将其从不均匀分布转换为在策略条件下的分布 (通常需要异步更新)，那么整个算法可以根据公式 (9.14) 确保收敛到一个有界的误差内。这一例证之所以受到如此关注，一是因为TD和DP算法是最简单和最直观的boostrapping算法，二则由于线性半梯度方法也是一种最简单和最直观的函数近似方法。这个例子表明了当backup与在策略的数据分布不一致时，即使是最简单的把boostrapping和函数近似相结合也会导致系统的不稳定。

​	也有类似于贝尔德反例一样的其它例子可以使得Q-learning算法发散不收敛。由于Q-learning有着所有控制类算法中最好的收敛性，这些例子受到了广泛的关注。在过去很多的研究学者试着去寻找不收敛问题的补救措施，或是尝试寻找一些更简单的确保收敛的条件。例如，只要行为策略 (用于选取动作的策略) 是诸如$\epsilon$-greedy一样与所估计的策略 (在广义策略迭代 GPI 中使用的策略) 足够接近的策略，那么确保Q-learning最终收敛是可行的。就我们目前所知而言，在这样类似的条件下还尚未发现过Q-learning不收敛的情况，但是目前尚没有足够的理论支撑这一假设。在本节后面的部分，我们将介绍几个其它被广泛研究的要点。

​	假使在贝尔德反例中，我们使用最小二乘近似并始终朝着最优的方向进行迭代，来代替每一次迭代过程中朝着一步回报的方向迭代，那么这可以解决不稳定的问题吗？如果特征向量$\{\phi(s):s\in\mathcal{S}\}$仍旧像贝尔德反例中相互之间是线性独立的话，那么答案是肯定的：在这一条件下我们可以精确地求解每一轮迭代的近似，并且整个算法退化成标准的表格情况下的DP (Dynamic Programming)。然而，实际上我们只能考虑无法精确求解的情况。在这一情况下，即使在每一次迭代都求解最优的近似仍旧不能保证系统的稳定性，下面给出一个具体的例证。 

![11-3](img\11-3.bmp)

​	图11.3: Tsitsiklis和Van Roy反例，通过最小二乘线性函数近似对 DP 策略进行估计。



**Example 11.1:  Tsitsiklis和Van Roy反例**        最简单的线性最小二乘DP反例如图11.3所示。存在两个非终止条件的状态，并且权重向量$\theta _k$是一个标量。对第一个状态值的估计是$\theta _k$，对第二个状态值的估计是$2\theta _k$。所有状态转移的奖励都是0，对于典型的$\theta _k=0$的情况，两个状态的真实值都是0。如果我们在每一步都计算使得估计值和期望的一步回报之间的MSVE误差达到最小，那么有：



$$\begin{align}\theta _{k+1} &\doteq\arg \min _{\theta\in \mathcal{R}}\sum _{s\in\mathcal{S}}[\hat{v} _\theta(s)-E_\pi[R_{t+1}+\gamma\hat{v} _{\theta _k}(S _{t+1})|S _t=s]]^2\\&=\arg \min _{\theta\in\mathcal{S}}[\theta-\gamma2\theta _k]^2+[2\theta-(1-\epsilon)\gamma2\theta _k]^2 \\&=\frac{6-4\epsilon}{5}\gamma\theta _k \end{align}$$ 

其中$\hat{v} _\theta$是近似的值函数。当$\gamma >\frac{5}{6-4\epsilon}$并且$\theta _0\ne0$时序列$\{\theta _k\}$将不会收敛。

​	通过使用特殊的函数近似是保证稳定性众多技术手段中的一种。尤其需要说明的是，对于不需要从观测目标进行推断的函数近似方法足以保证算法的稳定性。这些方法被称为中和器 (averagers)，包括最近邻法 (nearest neighbor methods) 和局部加权回归 (local weighted regression)，但是并不如贴片编码 (tile coding) 和反向传播 (backpropagation) 那样受到欢迎。



## 致命三要素##

当我们把以下三个要素相结合时，会大大提高系统不稳定和发散的风险：

1. 用于训练的状态集的分布与所估计的策略所产生的的数据集的分布不一致 (比如，离策略学习)
2. 尺度变化的函数近似 (比如，线性半梯度方法)
3. boostrapping (比如， DP、TD学习)

需要注意的是，不收敛的风险并不是由控制类的问题或是广义策略跌代 (GPI) 所导致的，它们在预测类的问题中同样存在。也不是由于学习类的方法导致的，在诸如动态规划的规划类方法中同样会发生发散。

​	此外，记住当存在三要素中的任意两个要素时算法是可以正常工作的；只有当三个要素都同时存在时才会导致不稳定的风险提高。



本章原著尚未完结...	    























 


