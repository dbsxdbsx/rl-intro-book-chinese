# 第15章
## 神经科学

神经科学是理解神经系统如何调控躯体功能、控制行为，如何随着发展、学习和衰老而变化，细胞和分子机制又如何产生上述功能的多学科领域。其中有关强化学习最为激动人心的方面就是神经科学有越来越多证据表明，人类和其他动物的神经系统采用一种和强化学习惊人相似的算法。本章的主要目标就是解释这种对应关系，以及通过这种对应关系说明动物的奖赏学习的神经基础。

强化学习和神经科学的最引人瞩目的联系在于多巴胺，一种涉及哺乳动物脑奖赏加工的化学物质，如何向负责学习和决策功能的脑结构传输时序差分（temporal-difference, TD）误差。这种对应关系被称为多巴胺神经元活动的奖赏估计误差假设。我们将在这一章讨论该假设，以及提出该假设的神经科学研究，还将讨论为什么这对于理解脑奖赏系统是一个巨大贡献。我们还要讨论强化学习和神经科学的其他联系，尽管这些联系可能不如多巴胺和TD误差的关系那么引人瞩目，但是也提供了构建动物奖赏学习概念的重要工具。
正如本书前言部分的历史回顾，强化学习的许多方面都受到神经科学的影响。因此本章接下来的第二个目标，就是向读者介绍影响我们对强化学习现有认识的那些脑功能。我们的计算观点有时候会让我们难以理解强化学习脑功能理论的渊源。特别是对于资格迹这个概念来说尤其如此，资格迹是强化学习的基本机制，来自于神经元突触的联结特性，而突触则是神经元细胞或者说神经元和其他神经元通讯的结构。
强化学习的其他方面有将来可能会影响神经系统的研究，但是目前而言和神经科学的联系认识还不够充分。这一章我们将讨论一些目前正在逐步揭示的、我们认为未来会更加凸显其重要性的那些联系。其中一个方面就是多智能体的强化学习，用于揭示交互作用的强化学习智能体集群的群体行为。尽管多智能体强化学习超出了本书的讨论范围，但是我们将触及该领域和神经科学的联系。另一个具有前景的强化学习和神经科学的交叉领域是计算神经科学，该领域旨在通过一些数学和计算方法，促进对于物质成瘾和精神疾病的理解，这些方法有的来自于强化学习。
目前有许多论及强化学习和神经科学交叉领域的学术出版物，有些将在本书最后一节引用到。我们的讨论将有别于大多数学术出版物，因为我们熟悉强化学习（从本书前面章节来说），但是对于神经科学并非如此。我们首先将简单介绍神经科学的概念，以便读者理解后面的部分（15.1节），并且讨论与神经科学和强化学习都有关联的神经元信号（15.2）。下一届我们还要讨论多巴胺神经元活动的奖赏估计误差假设，首先详细描述该假设（15.3），然后介绍脑多巴胺系统的研究背景（15.4），产生奖赏估计误差假设的科学研究（15.5），为什么TD误差和这些研究紧密相关（15.6）。接下来两节内容介绍脑如何利用多巴胺作为奖赏信号来执行行动者-评价者模型（15.7）。我们还要描述享乐神经元（hedonistic neurons），用神经科学的术语解释资格迹（15.9），接下来再介绍神经科学和强化学习智能体集群的潜在联系（15.10）。然后是神经科学的基于模型强化学习（15.11），以及强化学习如何受到成瘾和精神疾病研究的影响（15.12）。在概述和总结之后，书目和历史评论一节则会选择性地向读者推荐相关文献，并且增加历史细节（15.15）。
这一章我们没有深入探究动物奖赏学习有关的神经系统的复杂特性。因为篇幅所限，且我们并非神经科学家。我们没有试图描述或命名许多脑结构和通路，或者与这些过程有关的细胞机制。我们也没有对其他可以媲美强化学习的假设和模型给予足够的关注。我们只能浮光掠影地介绍这段引人入胜的、还在不断演绎的故事。不过我们仍然希望这一章能够让你相信，当前的研究已经在强化学习及其在动物神经科学奖赏学习领域的理论基础之间建立了丰富的联系。
**15.1神经科学基础**
为了理解贯穿本章的内容，我们有必要首先介绍下神经系统的基本信息。如果你对神经科学已经有了基础性的认识，可以略过本节。
神经元，是神经系统的基本组分，是利用电信号和化学信号加工及传输信息的特殊细胞。神经元有多种形式，但是一个神经元通常只有一个胞体，一个树突（dendrites），以及一个轴突。突触是神经元细胞体发出的、用于接收来自其他神经元传入信息的结构（或者也用于接收其他外界信号，这指的是感觉神经元）。神经元的轴突是携带神经元输出到其他神经元（或肌肉、腺体）的信息的纤维结构。神经元传出信息是一系列流经轴突的电脉冲，叫做动作电位（action potentials）。动作电位也叫做电脉冲（spikes），一个神经元只有产生电脉冲的时候才发放电位。在神经网络模型当中，通常使用实数来表征神经元的发放频率。
神经元轴突具有广泛的分支，以便动作电位能够通过神经元输出并到达多个目标。神经元轴突的分支结构叫做树状聚集样轴突（axonal arbor）。因为动作电位的传导是一个主动过程，不像是导线燃烧那样，当一个动作电位到达轴突分支的某一点时，会在所有的分支上“点燃”动作电位（尽管动作电位向某个分支的传播有时候会遭遇失败）。结果是一个具有大量树状聚集样轴突的神经元将影响许多目标。
一个突触通常位于一个轴突结束的地方，用于神经元向其他神经元的通讯。一个突触从突触前神经元传导信息到一个突触后神经元的树突或胞体。最有趣的是，来自于突触前神经元的动作电位一到达，突触就会释放叫做神经递质的化学物质。神经递质分子从突触的突触前部经过突触间隙（synaptic cleft），也就是突触前神经元末端到突触后神经元之间的微小间隙，然后与突触后神经元表面的受体结合，激活或抑制其产生脉冲的活动，或者以其他方式调控突触后神经元的活动。某种神经递质可以结合到不同类型的受体，且每种受体对于特定的突触后神经元产生不同的效应。例如，多巴胺至少可以通过五种神经递质影响突触后神经元。在动物神经系统已经发现了许多种不同的化学物质可以作为神经递质。
一个神经元的背景活动是活动水平，通常用动作电位发放频率表示，当该神经元没有受到实验者感兴趣的突触输入的影响时，例如当神经元活动和施加给被试的刺激无关时。背景活动可能没有规律，这是因为该神经元接收到来自神经网络的广泛输入，或者接收到该神经元或突触的噪音。有时候背景活动是由于内在于神经元的动态过程。神经元的相位性（phasic）活动，是相对于背景活动而言的，是由突触输入所导致的脉冲突然发放。无论是否有背景活动，神经元活动缓慢地变化并且强度不断升级，称为神经元的紧张性（tonic）活动。
神经递质在突触释放的强度或者有效性，对于突触后神经元的影响叫做突触的效能（efficacy）。神经系统可以通过经验而发生改变的途径之一，就是通过突触前神经元活动和突触后神经元活动的结合，从而改变了突触效能所致。这种突触效能的改变有时候还来自于神经调质（neuromodulator），这种神经元除了具有直接快速激活或抑制神经元的作用之外，还具有其他作用。
脑包括了不同的神经调质系统，而这些系统又包括了具有广大树状聚集样轴突的神经元簇，每个系统使用不同的神经递质。神经调质可以改变脑环路的功能，介导动机、唤醒、注意、记忆、心境、情绪、睡眠和体温。重要的是，神经调质系统可以传递标量信号如强化学习信号，从而改变对于学习至关重要的脑目标位置的突触活动。
突触改变其效能称为突触可塑性。突触可塑性是负责学习的基本机制。学习算法所调节的参数或权重，就对应于突触效能。我们接下来将详细介绍通过多巴胺递质来调控神经可塑性，是一种脑执行学习算法的合理机制。
**15.2奖赏信号，强化信号，价值和估计误差**

神经科学和计算领域的强化学习之间建立联系，首先是脑神经信号和强化学习理论及算法当中的信号建立联系。第三章我们提到学习目标指向行为的问题可以归结为三种信号：行动、状态和奖赏。然而，为了解释神经科学和强化学习之间的联系，我们会更具体地介绍这些内容，并且考虑强化学习信号和脑内其他时间信号之间的关系。除了奖赏信号，还包括强化信号（我们认为不同于奖赏信号）、价值和估计误差。我们是在强化学习理论的视角下通过其功能来标识信号的，信号是来自于等式或算法当中的术语。另一方面，当我们提到脑内信号的时候，我们指的是生理事件，比如动作电位的发放，或者神经递质的分泌。通过功能标记神经元信号，比如将多巴胺神经元的周期性活动叫做强化学习信号，意味着神经信号就像是理论意义上的信号一样发挥其功能。
发现这种神经信号和奖赏加工对应关系的证据面临很多挑战。奖赏加工有关的神经活动存在于大脑的很多部位，和奖赏有关的神经信号彼此高度相关，因此对于很多研究结果较难解释清楚。需要仔细设计实验，从而在某种程度上分离出和奖赏有关或者无关的信号。尽管存在上述困难，研究者已经做了很多实验，来整合和神经信号有关的强化学习理论和算法，也对神经信号和强化学习理论及算法之间的联系提出了强有力的证据。为了更好地考察这些联系，在本节内容当中我们要提醒读者根据强化学习理论来理解奖赏信号的含义，我们还初步论述了强化学习理论和神经信号之间的关系。
14.1节解释了为什么我们的提法是时刻t的奖赏信号Rt,，而不是时刻t的奖赏，因为它就像是动物脑中存在的信号一样，而不是环境当中的物体或事件。在强化学习理论当中，奖赏信号（以及智能体的环境）定义了强化学习智能体试图解决的问题。在这种意义上说，Rt的含义就像是动物脑将初级奖励信号传播到脑的某个部位。但是一个单独的起主导作用的奖赏信号不可能存在于动物脑中。最好将它理解为抽象的信号，代表着由脑中许多系统产生的大量神经元信号的整体效应，这些系统用于评估感觉和状态的奖励或者惩罚特性。
强化学习当中的强化信号和奖赏信号不同。强化信号的功能是通过学习算法，引导智能体的策略、估计价值或环境模型进行改变。我们可以用很多方式来定义强化信号，但是最有意义的，也最适合心理学和最适合神经科学的，是用有符号的标量，例如某个正数或负数，乘以一个矢量（再加上一些常数），使学习系统得以更新参数。比如对于TD系统，强化信号就是TD误差。标量对学习过程最为关键。我们将学习算法当中发挥类似作用的、有符号的标量看做是强化信号。奖赏信号可能发挥类似强化信号的功能，然而对于我们考虑范围内的大多数算法，其强化信号是受到其他信息所调节的奖赏信号，比如TD误差当中的价值估计。
估计状态价值或行动价值，比如V或者Q，规定了长期来讲怎么样对智能体是好的、怎么样是不好的。状态/行动价值是智能体对于未来能够预计到的最大累积价值。智能体通过选择行动来达到具有最大估计状态价值的状态，或者选择具有最大估计行动价值的行动。
估计误差测量的是预期信号和实际信号或者观察之间的差异。奖赏估计误差（RPEs）测量的是预期和实际的奖赏信号之间的差异，当奖赏信号大于预期的时候奖赏估计误差是正的，当奖赏信号小于预期的时候奖赏估计误差是负的。TD误差是特殊的奖赏估计误差，代表当前的预期奖赏和较早的预期奖赏长期的差异。（TD误差还可以被智能体用于学会如何预测奖赏以外的其他特征，不过我们这里不讨论这种情形，读者可以参考 Modayil, White, and Sutton, 2014）当神经科学家指奖赏估计误差的时候他们一般指TD奖赏估计误差，我们在本章中简称TD误差。本章的TD误差一般而言指的是不取决于行动的，不同于利用Sarsa和Q学习算法学习行动价值时所采用的TD误差。这是因为大多数强化学习和神经科学的联系都是用无行动的TD误差来表述的，但是我们不是说要排除掉取决于行动的TD误差。
神经科学的数据和理论所定义的信号之间的联系，我们会想到许多问题。比如是不是观察到的信号更像是奖赏信号，一个价值，一个估计误差，一个强化信号，或者其他截然不同的东西？是不是误差信号，是不是RPE，是不是TD误差，或者其他更简单的误差比如Rescorla-Wagner误差？如果是TD误差，是否像是Q学习或者Sarsa算法当中的TD误差一样取决于行动？如上所述，通过探测大脑来回答上述问题是极其困难的。但是实验证据提示，某种神经递质，具体来说多巴胺反映着RPE信号，进一步地，分泌多巴胺的神经元的相位活动会传递TD误差（见15.1对于相位活动的定义）。该实验证据产生了多巴胺神经元活动的奖赏估计误差假设，我们接下来将加以描述。
**15.3 奖赏估计误差假设**
多巴胺神经元活动的奖赏估计误差假设假设，哺乳动物产生多巴胺的神经元，其相位活动的功能之一就是传递对预期未来奖赏的之前的估计和新的估计之间的误差。这个假设（尽管不是原话）最初是由 Montague、Dayan和Sejnowski（1996）提出的，他们说明了强化学习的TD误差概念可以解释哺乳动物多巴胺神经元的相位活动的许多特征。导致该假设提出的一系列实验则是由神经科学家Wolfram Schultz在20世纪八九十年代在自己的实验室做出的。15.5节将描述这些影响深远的实验，15.6节将解释这些实验结果和TD误差的吻合之处，章末的参考文献和历史评述将向读者推荐和该假设的发展有关的文献。
Montague等（1996）比较了经典条件作用TD模型的TD误差在模拟试验当中的变化，和类似任务当中动物多巴胺神经元的相位活动。回忆14.3，经典条件作用的TD模型基本上就是用线性函数逼近的梯度下降的TD(λ)算法。为了便于比较，Montague提出了若干假设。第一，既然TD误差可以为负，他们就假定对应于多巴胺神经元活动的数量为δt +bt，而bt是神经元的背景发放频率。负的TD误差对应于多巴胺神经元发放频率下降到背景频率以下。和奖赏估计误差假设的文献一致，本章当中δt 指的是t时刻具有的TD误差，而不是t时刻做出的估计所产生的误差（因此和6.5节一样，δt指的是δt-1）。
第二个假设是关于经典条件作用实验当中访问的状态，是如何在学习算法当中表征为输入的。这也是我们在14.3.3当中讨论经典条件作用的TD模型所提到的。Montague等选择了完整的序列复合（CSC, complete serial compound）表征，如图14.2左栏所示，但是持续时间较短的内部信号序列会持续到无条件刺激开始的时候，直到出现奖赏信号。这样的表征方式使得TD误差能够模拟多巴胺神经元的活动，即不仅能够预测未来会发生奖赏，而且对可以预测奖赏刺激来临的线索也敏感。有一些方法可以追踪感觉线索和奖赏刺激到来之间的时间间隔。如果一个刺激启动了一系列内部信号，并持续到刺激结束，如果刺激呈现之后每一个时间点都有一个不同的信号，那么刺激呈现之后每一个时间点都可以用一个离散状态加以表征。因此，TD误差取决于状态，也就是说对试验当中的事件发生时间敏感。
由于对经典条件作用的TD模型模拟试验，假定有上述关于背景发放频率和输入表征的假设，TD误差和多巴胺神经元活动之间就有了如下的共同特征：（1）只有当超出预期的奖赏事件发生时才会产生多巴胺神经元的相位活动；（2）学习早期，奖赏之前的神经线索不会导致大量的多巴胺神经元相位活动，但是随着学习持续，这些线索就有了预测价值，可以引发多巴胺神经元相位活动；（3）如果更早发生的线索，后面总是跟随一个具有预测价值的线索，那么较早的线索逐渐也会引发多巴胺神经元的相位活动，而后面那个线索引发的多巴胺神经元相位活动会停止；（4）当个体学会了的、预期的奖赏事件没有发生，那么多巴胺神经元的活动就会在奖赏事件预期会发生的时间点降至基线水平以下。
尽管在Shultz及其同事的实验当中，不是监测到的每个多巴胺神经元都以这种方式运作，但是大多数神经元活动和TD误差之间惊人的对应关系，仍然有力地支持了奖赏估计误差假设。但是还有一些情况，假设和实验观察不相符。TD误差和大多数多巴胺神经元活动之间的联系有多紧密，输入表征的选择很关键，特别是多巴胺神经元反应的时间。我们接下来也会提到，研究者提出了诸多设想用来说明TD学习模型的输入表征和其他特征，以便让TD误差和数据之间拟合得更好，当然最主要的拟合关系，还是Montague等提出的CSC表征当中TD误差和多巴胺神经元活动之间的拟合关系。总体而言，奖赏估计误差假设被研究奖赏学习的神经科学家广泛接受，并且在大量的神经科学实验结果面前显示了仍然保持了其基本特性。
为了描述支持奖赏估计误差假设的神经科学实验，并提供一些背景资料使读者能够体味该假设的重要之处，我们接下来将介绍多巴胺及其有关的脑结构，以及多巴胺在奖赏学习当中的作用。
**15.4多巴胺**
多巴胺一种神经递质，产生多巴胺的神经元其胞体主要存在于哺乳动物中脑的两大神经元簇：黑质致密部(substantia nigra pars compacta, SNpc) 和腹侧被盖区(ventral tegmental area, VTA)。多巴胺在哺乳动物脑的许多功能当中具有实质性的作用。尤其是动机、学习、动作选择，以及大多数类型的物质成瘾、精神分裂症和帕金森等。多巴胺之所以被称为神经递质，是因为不仅能够快速激活或抑制靶向神经元。尽管多巴胺的功能及其细胞学效应还有大量未解之谜，但是可以明确的是多巴胺是哺乳动物脑奖赏加工所需的基本物质。多巴胺不是唯一负责奖赏加工的神经递质，多巴胺在厌恶刺激情形——如惩罚——当中的作用还充满争议。多巴胺对于非哺乳动物来说功能也有所不同。但是没有人会怀疑多巴胺在哺乳动物包括人的奖赏加工当中的必要作用。
传统观点认为多巴胺神经元对参与学习和动机过程的大量脑区广泛发放奖赏信号。这种观点来自于James Olds和Peter Milner在1954年发表的一篇著名文献，该文献描述了对大鼠脑的某些区域进行电刺激的后果。他们发现电刺激某些脑区的作用类似于强烈的奖赏刺激，并能够控制大鼠行为：“……这种奖赏刺激对动物行为产生了极强的控制作用，可能超过了以外实验当中的任何一种奖赏刺激”（Olds and Milner, 1954）。后来的研究揭示了电刺激最有效的脑部位会直接或间接地激活多巴胺通路，而多巴胺通路通常是被自然状态下的奖赏刺激所激活的。类似的效应也出现在人类被试上面。这些实验观察都强烈暗示了多巴胺神经元可以产生奖赏信号。
但是奖赏估计误差假设如果正确的话，即便只能解释多巴胺神经元的部分活动，那么多巴胺神经元可以产生奖赏信号的传统假设就不能说是完全正确的：因为多巴胺神经元的相位活动提示的是奖赏估计误差，而不是奖赏本身。用强化学习的术语来说，多巴胺神经元的相位活动对应于δt而不是Rt。

强化学习理论和算法有助于将奖赏估计误差假设和传统的多巴胺提示奖赏信号的观点进行调和。本书探讨的许多算法当中，δt是作为强化信号的，也就是说是学习的主要驱动者。例如δt是经典条件作用TD模型的关键因素，在评价者-行动者模型当中是学习价值函数和策略的强化信号（13.5和15.7节），和行动有关的δt则包括Q学习和Sarsa。奖赏信号Rt对于δt是关键的成分之一，但在这些算法当中不是完全决定δt强化作用的因素。
仔细考察Olds和Milner在1954年发表的论文，可以明白该论文实际上主要讨论的是工具性条件任务当中的电刺激的强化作用。电刺激不仅通过多巴胺的动机作用，让大鼠的行为活力更强，还导致了大鼠能够迅速学会自行按压杠杆刺激自己，并且在相当长时间内乐此不疲。电刺激诱发的多巴胺神经元活动强化了大鼠按压杠杆的行为。
进一步明确了多巴胺神经元的相位活动作为强化信号的，是近期的一系列光遗传学实验。这种方法可以使神经科学家在毫秒级时间尺度上准确控制清醒动物某种类型的神经元。光遗传学方法将光敏蛋白结合到选定类型的神经元当中，从而使这些神经元就可以在闪光或者激光的作用下激活或者沉默。光遗传学的第一个实验就是研究了多巴胺神经元，表明了光遗传学刺激导致了小鼠多巴胺神经元的相位活动，并作为条件刺激使小鼠产生对实验箱一侧位置的偏好，而对另一侧不接受光刺激或者接受低频刺激的位置，小鼠则没有习得这种偏好（Tsai et al. 2009）。另一个例子是Steinberg（2013）年用光遗传学技术刺激多巴胺神经元对预期的奖赏刺激产生虚拟放电，而在预期奖赏刺激未发生的时候，像平时一样停止放电。当停止放电被虚拟放电代替的时候，多巴胺神经元的反应并没有像缺乏强化时的那样有所消退，即反应下降，而是得以维持。当预期奖赏已经发生、而常规情况下神经元活动应当被阻滞的时候（阻滞实验见14章）。
关于多巴胺在强化中的功能，果蝇的光遗传学实验也提供了证据，哺乳动物的多巴胺功能和果蝇正好相反：光所触发的多巴胺神经元活动发放，和用于强化回避行为的足底电击所引发的神经元活动是类似的，至少对于激活的多巴胺神经元簇是如此（(Claridge-Chang et al. 2009）。尽管这些光遗传学实验都没有显示多巴胺相位性活动和δt的关联，但是都显示了多巴胺神经元的相位性活动，其作用相当于控制（工具性条件作用）和预测（经典条件作用）算法当中作为强化信号的δt。
多巴胺神经元特别适合向大脑许多区域传播强化信号。多巴胺神经元具有巨大的树状聚集样轴突，每个轴突都向大量其他神经元突触释放多巴胺，其数量相当于普通轴突的100~1000倍。图15.1显示了胞体位于小鼠脑SNpc区域的某个多巴胺神经元的树状聚集样轴突。VTA或者SNpc的多巴胺神经元所触及的靶向脑区的树突数量大约有500，000个。
如果多巴胺神经元如同强化学习的δt那样传播强化信号，那么这个信号应该是一个标量信号，比如某个数量，SNpc和VTA所有的多巴胺神经元必须以几乎同样的方式激活，以便能够同时发放神经冲动，并释放同一个神经信号到所有的靶向神经元。尽管人们普遍相信多巴胺神经元是以这样的方式共同活动的，当前的证据所反映出来的情况却更为复杂，不同的多巴胺神经元集合对于刺激输入的反应也是不同的，其反应取决于发送信号的神经结构。多巴胺神经元的活动和单一的强化信号标量并不是对应的关系，从强化学习的角度来看这是有意义的——不过这个问题超出了本书的范围。这可能是一种在神经结构层面上解决信度分配问题的方式，基于系统的不同成分及其导致的结果，奖赏信号将指向系统的特定成分。我们还将在15.10部分进行详述。
大部分多巴胺神经元的轴突和额叶、基地神经节的神经元都有突触上的联系，这些脑区涉及主动运动、决策、学习和认知功能比如计划。由于大多数将多巴胺和强化学习相联系的研究都关注基地神经节，同时该区域的多巴胺神经元非常致密，因此我们此处也主要关注基地神经节。基地神经节是位于前脑底部的神经元集合或者神经核。基地神经节的主要传入结构叫做纹状体。几乎大脑的绝大部分区域都对纹状体有传入神经。大脑皮层神经元的活动携带大量感觉输入、机体内部状态和运动活动的信号。皮层神经元的轴突与纹状体主要输入输出神经元的树突（叫做中型棘突神经元，medium spiny neurons）有突触上的联系。纹状体的传出神经再次经过基底神经节的其他核团以及丘脑，然后传导至额叶及运动区，这使得纹状体能够影响运动、抽象决策和奖赏加工过程。纹状体的两个主要亚区对于强化学习具有重要影响：背侧纹状体，主要参与行动选择；腹侧纹状体，对于奖赏加工的很多方面至关重要，比如对特定感觉赋予情绪价值。
中型棘突神经元的树突覆盖着一层突起，其末端和皮层神经元的轴突相接触。多巴胺神经元的轴突也会接触这些突起——这里指的是突起的主干部位（图15.2）。这种组织方式能够将皮层神经元的突触前活动、中型棘突神经元的突触后活动以及来自多巴胺神经元的输入整合在一起。而这些突起部位到底发生了什么，问题复杂而难以理解。从图15.2可以看出皮层输入的两种神经递质受体——多巴胺和谷氨酸盐，以及多种信号之间的复杂作用。不过目前有越来越多证据显示，从皮层到纹状体的突触效能的变化，取决于具有某种时间特性的多巴胺信号。

#### 15.5奖励预测误差假设的实验支持

多巴胺神经元对激烈的，新颖的或意想不到的触发眼睛和身体运动的视觉和听觉刺激具有活动的反应，但是它们的活动很少与运动本身有关。这是令人惊讶的，因为多巴胺神经元的退化是帕金森病的主因，其症状包括运动障碍，特别是自发性运动的缺陷。由于多巴胺神经元活动与刺激引发的眼睛和身体运动之间的弱关系，罗姆和舒尔茨（1990 ）及舒尔茨和罗姆 (1990)通过记录猴子移动它们的手臂时多巴胺神经元活动和肌肉活动而迈向奖励预测误差假说的第一步。

![Figure15.2](D:\trans\rl-intro-book-chinese\chapter15\img\Figure15.2.png)

图15.2：纹状体介质多刺神经元的脊柱，显示皮质和多巴胺神经元的输入。 皮质神经元的轴突通过突触覆盖这些神经元的树突的脊柱尖端通过纹状体介质多刺神经元。 *VTA*或*SNpc*多巴胺神经元的每个轴突与其经过的大约$500,000$个脊柱的茎突触接触，其中多巴胺从“多巴胺静脉曲张”中释放出来。 这种安排将来自皮层的突触前输入，中等多刺神经元的突触后活动和多巴胺，使得几种类型的学习规则可以控制皮质脊髓束突触的可塑性。 From Journal of Neurophysiology，W.Schultz，vol。 80，1998，第10页。

​     他们训练了两只猴子看到并听到垃圾箱的门打开时从静止状态伸手到一个包含一点苹果，一块饼干或葡萄干的箱子里。猴子可以抓住并把食物放到嘴里。在一只猴子变得熟练之后，另外给它两项额外的训练任务。第一个任务的目的是看到当运动是自发的，多巴胺神经元做什么。这个垃圾桶是打开的，但是从上面盖上了，所以猴子看不到里面，但是可以从下面可以伸手进去。没有出现触发刺激，当猴子摸到和吃了食物后，实验者通常（虽然不总是），悄悄的不让猴子看见，通过用铁丝来串着食物来替换之前食物直接放置在箱子里。在这里，罗姆和舒尔茨监测的多巴胺神经元的活动与猴子的活动不相关，但是当猴子首先触及食物的时候，这些神经元的很大比例产生了相位响应。当猴子只触摸铁丝或探索空的箱子时这些神经元没有响应。 这是证据显示神经元只对食物的反应，而不是对任务的其他方面。

​    罗姆和舒尔茨的第二个任务的目的是看看当运动被刺激触发时会发生什么。 这个任务使用了一个可打动盖子的箱子。垃圾桶开口的画面和声音触发伸手到箱子的活动。在这种情况下，他们发现经过一段时间的训练后，多巴胺神经元对触摸食物没有反应，而是响应食物箱盖开口的画面和声音。这些神经元的阶段性反应已经从奖励本身转移到预测奖励的可用性的刺激。在随后的研究中，他们发现被监测的大多数多巴胺神经元的活动没有回应在任务背景之外的开盖的画面和声音。这些观察结果表明，多巴胺神经元对启动运动或刺激感觉特性方面不作出反应，而是表现出对奖励的期望。

​     舒尔茨的小组还进行了许多涉及*SNPc*和*VTA*多巴胺神经元的研究。特定的一系列实验有意义地表明多巴胺神经元的相位相当于*TD*误差，而不是像*Rescorla-Wagner*模型（14.3）中的那些简单的错误。在第一次这些实验（Ljungberg，Apicella和Schultz，1992）中，猴子被训练以在灯被点燃作为“触发提示”然后压下杠杆以获得一滴苹果汁。正如罗姆和舒尔茨早些时候所观察到的，许多多巴胺神经元最初对奖励 - 滴落的果汁作出了回应（图15.3，上图）。但是，随着训练的继续和不断发展，许多神经元将灯亮这个预测的奖励信号进行反应而不是果汁（图15.3，中图）。随着训练的持续，杠杆按压变得更快，而对响应触发提示的多巴胺神经元数量在减少。

​      在这项研究之后，同一批猴子接受了一项新任务的训练（Schultz，Apicella和Ljungberg，1993）。这里的猴子面临着两个杠杆，每个上面都有灯。 亮灯其中之一是“指示提示”，指示两个杠杆中的哪一个将会产生一滴苹果汁。在这个任务中，在前一任务的触发提示之前的指令提示有1秒的固定间隔。猴子学会等待直到看到触发提示，并且多巴胺神经元活动增加，但现在监测的多巴胺神经元的反应几乎完全发生在较早的指令提示，而不是触发提示（图15.3，下图）。在任务得到良好的掌握之后，再次响应指令提示的多巴胺神经元的数量大大减少。在学习这些任务期间，多巴胺神经元活动从最初响应到奖励转移到响应早期的预测刺激，首先进行到触发刺激，然后进行到更早的指令提示。由于回应早些时候，它从后期的刺激中消失。 这种对早期奖励预测因素的反应转变，而对后期预测变量的反应却是TD学习的一个标志（例如见图14.5）。

​      刚刚描述的任务揭示了与TD学习与多巴胺神经元活动相似的另一个属性。 猴子有时会按下错误的按键，也就是非指示的键，因此没有奖励。在这种情况中，许多多巴胺神经元在奖励通常的发生时间之后不久就显示出低于基线的发射率的急剧下降，而且没有任何外部提示可以标记通常的奖励发生时间（图15.4）。

---

![Figure 15.3](D:\trans\rl-intro-book-chinese\chapter15\img\Figure 15.3.png)

图15.3：多巴胺神经元的反应从初始反应转移到初级奖励到早期预测刺激。 这些是在试验期间在指定时间产生阶段反应的监测的多巴胺神经元数量的图。 顶部：多巴胺神经元被不可预测的苹果汁滴落所激活。 中间：随着学习，许多多巴胺神经元产生了对奖励预测触发提示的响应，并且失去了对交付的奖励响应。 底部：将触发提示之前的指令提示加1秒，许多多巴胺神经元将其响应从触发提示转移到较早的指令提示。 来自舒尔茨等人 （1995），麻省理工学院出版社。

不知怎的，猴子在内部跟踪奖励的时间。 （响应时间是一个区域，其中最简单的*TD*学习版本需要修改，以解释多巴胺神经元反应时间的一些细节，我们在下一节中考虑这个问题）

​      上述研究的观察结果使得舒尔茨及其研究小组得出结论，多巴胺神经元对不可预测的奖励作出了回应，而对奖励的最早预测因素作出了回应，如果奖励或奖励预测值在其预期时间内不发生，则多巴胺神经元活性降低到基线以下 。熟悉强化学习的研究人员很快意识到，这些结果与*TD*算法中*TD*误差$\delta_t$作为加强信号的行为非常相似。下一节通过详细的具体实例来探讨这种相似之处。

#### 15.6 *TD*误差/多巴胺对应关系

   本节的目的是解释在前面部分描述的实验中观察到的TD误差$δ_t$与多巴胺神经元的相位响应之间的对应关系。

---

我们看一下在任务中如何改变$\delta_t$的学习过程，如上所述，猴子第一次看到指令提示，然后一段固定的时间必须正确地响应触发提示以获得奖励。我们使用这个任务的简单理想化版本，但是我们比通常更关注细节，因为我们要强调*TD*误差和多巴胺神经元活动之间的平行的理论基础。

![Figure 15.4](D:\trans\rl-intro-book-chinese\chapter15\img\Figure 15.4.png)

图15.4：预期奖励没有发生之后时间，多巴胺神经元的反应降低到基线以下。 顶图：多巴胺神经元通过一滴不可预测的苹果汁的滴落而被激活。 中图：多巴胺神经元对预测奖励的条件刺激（*CS*）作出反应，但不回应奖励本身。 底图：当*CS*预测的奖励不能发生时，在预计发生奖励的时间之后，多巴胺神经元的活动将会下降到基线以下。 在每个这些面板的顶部显示在试验期间在指定时间产生阶段响应的监测的多巴胺神经元的数目。 下面的光栅图显示了被监测的单个多巴胺神经元的活动模式。 来自Schultz，Dayan和Montague，A Neural Substrate of Prediction and Reward，Science，vol。 275，第5306号，第1593-1598页，1997年3月14日。经*AAAS*许可转载。

​      第一个简化的假设是智能体已经学会了为获得奖励而采取行动。 那么它的任务只是为了学习准确预测它所经历的状态序列的未来奖励。

---

在强化学习的术语中，这是预测任务，或更技术化的称为策略评估任务：

学习固定策略的值函数的任务（第4.1节和第??节）。要学习的价值函数为每个状态分配一个值，该状态预测将遵循该状态的返回值，如果智能体根据给定的策略选择动作，其中返回是所有未来奖励信号的（可能是损耗的）总和。这是不现实的，因为猴子的情况是一个模型，因为猴子可能会在学习正确行为的同时学习这些预测（以及学习策略以及价值函数的强化学习算法，例如演员 - 评论家算法），但是这种场景比同时学习策略和值函数的场景更简单。

​     现在想象一下, 智能体的经验分为多个试验, 其中每一个都是相同的状态序列重复, 在试验的每一步都有一个不同的状态发生。进一步想象，预测的回报被限制在通过一个试验的回报，这使得试验类似于我们定义的强化学习情节。 实际上，当然，所预测的回报并不局限于单次试验，试验之间的时间间隔是决定动物学习的重要因素。*TD*学习也是如此，但在这里我们假设回报不是累积多次试验的累积。因此，那么，如同舒尔茨及其同事所进行的一系列的试验就等同于强化学习。 （虽然在这个讨论中，我们将使用术语试验而不是情节会更好地与实验联系起来。）

​     像往常一样，我们还需要假设如何将状态表示为学习算法的输入，这是假设*TD*误差与多巴胺神经元活动相当接近。 我们稍后讨论这个问题，但是现在我们假设蒙塔古等人使用相同的*CSC*表示。（1996年），在情节的每个时间步骤中，每个状态都有独立的内部刺激。 这将本书第一部分所述的过程减少到能被表格表示的情况。 最后，我们假设智能体使用*TD*（0）来学习一个价值函数，$V$ ， 存储在初始化为零的查找表中的所有状态。 我们还假设这是一个确定性的任务，衰减因子$\gamma$非常接近于1，因此它可以被忽略。

​       图15.5 显示了在这个策略评估任务中学习的几个阶段的$R$、$V $和$\delta$ 的时间进程。图中的顶图表示每个试验中访问的状态序列（其中不是表示离散状态，我们只显示它们被访问的时间间隔）。每个试验中的奖励信号为零，除非智能体达到奖励状态，当时间线的右端附近显示奖励信号时，奖励信号变为一些正数，即$R^*$。 *TD*学习的目标是预测在试验中访问的每个状态的回报，这种完全的情况下，假设我们假设预测仅限于个别试验，对于每个状态来说，只是$R ^ *$。

​        在奖励状态之前是一系列奖励预测状态，其中最早的奖励预测状态显示在时间线左端附近。这就像在试验开始的状态，例如在上文描述的舒尔茨等人的猴子试验中由指示提示标示的状态（1993）。

---

试验中的第一个状态可靠地预测了试验的回报。（当然，事实上，在以前的试验中访问过的状态甚至是早期的奖励预测状态，但是由于我们将预测限于个别试验，所以这些不符合本试验回报的预测因素。）下面我们给出一个更令人满意的，尽管更抽象的描述最早的奖励预测的状态。试验中最新的奖励预测状态是试验有效状态之前的状态。 这是图15.5中时间线最右端附近的状态。（请注意，试验的回报状态并不能预测该试验的回报：这个状态的价值将来预测所有以下试验的回报，在这里我们假设这个情节公式为零。）

![Figure15.5](D:\trans\rl-intro-book-chinese\chapter15\img\Figure15.5.png)

图15.5：$TD$学习期间（可用）$TD$误差$\delta$的行为与多巴胺神经元的相位激活的一般特征一致。上图：状态序列，以正常预测因子的间隔显示，后面是非零回报信号$R$。学习早期：初始值函数$V$和初始值$δ$起始等于$R$. 学习完成：价值函数准确预测未来的奖励，$\delta$在最早的预测状态为正，在非零奖励时$\delta = 0$。 省略$R$：在省略预测奖励时，$\delta$变为负数。 有关完整说明，请参阅文本。

​       图15.5 显示了作为“早期学习”图标签的$ V$ 和 $\delta$第一次试验内容。因为除了达到奖励状态以及所有V值都为零之外，整个试用期间的奖励信号为零，所以TD误差也为零，直到在奖励状态下变为$R^*$。这是因为$\delta_t = R_t + V_t -V_t-1 = R_t + 0 -0 = R_t$，它为零直到当奖励发生时它等于$R^*$。

---

这里$V_t$和$V_{t-1}$分别是在试验中在时间$t$和$t-1$访问的状态的估计值。在这个学习阶段的TD误差类似于多巴胺神经元在训练开始时响应不可预测的奖励，如一滴滴落的苹果汁。

​      在第一次试验和所有连续试验中，$TD(0)$备份发生在每个状态转换中，如第6章所述。随着增加从奖励状态向后扩展，直到值收敛到正确的返回预测值，这继续增加了预测奖励状态的值。在这种情况下（因为我们假定没有损耗）正确的预测是等于$R^*$ 对于所有的预测奖励状态。在图15.5中可以看出，$V$标记为“学习完成”的图形，其中从最早到最新的预测奖励状态的所有状态的值都等于$R^*$ 。最早的预测奖励状态之前的状态值仍然很低（图15.5显示为零），因为它们不是可靠的奖励预测因子。

​      当学习完成时，也就是说，当$V$达到正确的值时，与任何奖励预测状态的转换相关的TD错误为零，因为当前的预测是准确的。这是因为对于从预测奖励状态到另一种预测奖励状态的转换，我们有$\delta_t = R_t + V_t-V_{t1} = 0 + R^*-R^* = 0$，并且对于从最新的预测奖励状态到奖励状态的转变，我们有$\delta_t = R_t + V_t-V{t_1} = R^* + 0- R^* = 0$。然而，由于该状态的低值与以下的预测奖励状态的较大的值之间的不匹配，从任何状态到最早的预测奖励状态的转变的TD误差是正的。实际上，如果在最早的预测奖励状态之前的状态的值为零，则在转换到最早的预测奖励状态之后，我们将使得$\delta_t = R_t + V_t -V_{t-1 }= 0 + R^* -0 = R^* $图15.5中$\delta$的“学习完成”图显示了最早的预测奖励状态下的正值，其他地方为零。

​    在转换到最早的奖赏预测状态时, 正 $TD$ 误差类似于多巴胺对最早的刺激预测奖励的持续反应。 同样，当学习完成时，从最新的奖励预测状态到有奖励状态的转变产生零$TD$误差，因为最新的预测奖励状态的值正确，取消了奖励。 这与观察结果相似，相对于不可预测的奖励，完全预测的奖励多巴胺神经元的阶段性反应更少。

​      如果经过学习，奖励突然被忽略，则通常的奖励时间$TD$误差为负，因为最新的预测奖励状态的值太高：$\delta_t = R_t + V_t -V_{t-1} = 0 + 0- R^* = -R^*$， 如图15.5右端所示的“省略的$R$”图的$\delta$。这就像舒尔茨等人的实验（1993）所示：多巴胺神经元活性下降在基线以下当预期奖励消失的时候，如图15.4所示。

​       最早的奖励预测状态的想法值得更多的关注。 在上述情况下，由于试验的经验，我们假设预测仅限于个别试验，最早的预测奖励状态始终是试验的第一个状态。 显然这是人为的。考虑最早的预测奖励状态的更一般的方式是，它是一个不可预测的奖励预测因素，可以有很多这样的状态。在动物的生命中, 许多不同的状态可能在最早的预测奖励状态之前。然而, 由于这些状态往往被其他不预测奖励的状态所效仿, 他们的奖励预测能力, 即他们的价值, 仍然很低。$TD$算法, 在整个动物的生命中运行, 也会将值备份到这些状态, 但是备份不会持续累积, 因为根据假设, 这些状态都不能在最早的预测奖励状态之前可靠地进行。如果他们中的任何一个, 他们也将是奖励预测的状态。这也许可以解释为什么过度训练, 多巴胺反应甚至减少到试验中最早的预测奖励刺激。在过度训练的情况下, 人们会认为, 即使以前不可预测的预测状态将由与先前状态相关的刺激预测： 动物在实验任务内外的相互作用将变得司空见惯。然而, 在打破这一例行的新任务的引入, 你会看到 $TD$ 错误再现, 确实是观察到多巴胺神经元活动。

​       本节中描述的示例说明了当动物正在学习类似于我们示例的理想化任务时，$TD$错误与多巴胺神经元的相位活动共享关键特征的原因。 但是，并不是多巴胺神经元的相位活动的所有性质都与$\delta$的性质完全一致。最令人不安的差异之一是当奖励比预期更早发生时会发生什么。 我们已经看到，省略预期的奖励会在奖励的预期时间产生负的预测误差，对应于这种情况发生时, 多巴胺神经元活动减少到基线以下。 如果奖励迟于预期，那么这是一个意想不到的回报，并产生一个正的预测误差。这在$TD$误差和多巴胺神经元反应都发生了。 但是当奖励早于预期时，多巴胺神经元不会做TD错误的做法——至少蒙塔古等人使用的$CSC$表示 （1996）和我们的例子是这样。 多巴胺神经元对早期的奖励做出回应，这与正面的TD错误是一致的，因为没有预测到奖励发生。 然而，在预期出现但省略奖励的较后时间，$TD$误差为负数，但多巴胺神经元活动不会以$TD$模型预测的方式下降到基线以下（Hollerman和Schultz，1998）。 动物大脑比就是简单地用$CSC$表示的$TD$学习中更复杂。

​         $TD$误差和多巴胺神经元活性之间的一些不匹配可以通过选择$TD$算法的合适参数值和通过使用不同于$CSC$表示的刺激表示来解决。 例如，为了解决上述早期奖励不匹配，Suri和Schultz（1999）提出了一种$CSC$表示法，其中早期刺激发起的内部信号序列由于奖励的发生而被取消。 Daw，Courville和Touretzky（2006）的另一个建议是，大脑的$TD$系统通过在感觉皮质中进行的统计建模产生的表示，而不是基于原始感觉输入的简单表示。Ludvig，Sutton和Kehoe（2008）发现，使用微刺激（MS）表征的$TD$学习（图14.2）比使用$CSC$表示更适合多巴胺神经元在早期报酬和其他情况下的活动。Pan，Schmidt，Wickens和Hyland（2005）发现，即使使用$CSC$表示， 延长的资格跟踪改善了在某些方面的多巴胺神经元活动$TD$误差的适应性。一般来说，$TD$误差行为的许多细节都取决于资格追踪，损益和刺激表示之间的微妙互动。 这样的结果精心设计并完善了奖励预测误差假说，而不驳斥其核心主张，即多巴胺神经元的相位活动被充分表征为传递$TD$误差信号。

​       另一方面，$TD$理论和实验数据之间还存在其他差异，这些差异不是通过选择参数值和刺激表示来就可以容易地修复的（我们在本章末尾提到“参考文献和历史备注”部分中的一些差异） ，并且更多的错配可能被发现，由于神经科学家进行了更精细的实验。但是，奖励预测误差假说已经非常有效地作为促进我们对大脑奖励制度如何运作的理解的催化剂。 已经设计了复杂的实验来验证或反驳从假设得出的预测，而实验结果又导致TD误差/多巴胺假说的改进和阐述。

​      这些发展的一个显著的方面是, 强化学习算法和理论, 连接这么好的多巴胺系统的性质是从一个计算的角度, 完全没有任何关于多巴胺神经元相关特性的知识-记忆, $TD$学习及其与最优控制和动态规划的关系, 是在进行任何实验之前的几年中发展起来的, 它揭示了多巴胺神经元活动的$TD$ 性质。这种无计划的通信, 尽管不是完美的, 表明$TD$ 误差/多巴胺并行捕获的一些重要的大脑奖励过程。

​     除了考虑多巴胺神经元的相位活动的许多特征之外，奖励预测误差假说将神经科学与强化学习的其他方面联系起来，特别是将$TD​$误差用作加强信号的学习算法。神经科学尚未完全了解多巴胺神经元的相位活动的电路，分子机制和功能，但支持奖励预测误差假说以及相位多巴胺反应是学习的强化信号的证据表明， 大脑可能会实施类似于演员—评论家算法, 其中$TD​$误差扮演着关键角色。其他的强化学习算法也是可行的。但是演员—评论家算法特别适合哺乳动物脑的解剖学和生理学，如下面两节所述。

#### 15.7 神经演员—评论家算法

​     演员—评论家算法学习策略和价值功能。“演员”是学习策略的组成部分，“评论家”是了解演员目前正在遵循的任何策略的组成部分，以批评演员的行动选择。

---

评论家使用$TD​$算法学习与演员当前策略相关联的状态—值函数。 价值函数允许评论家通过向演员发送$TD​$误差$\delta​$来批评演员的行动选择。 正数$\delta​$意味着行动是“好”，因为它导致了一个好于预期值的状态; 负$\delta​$”意味着行动“坏”的，因为这导致了一个低于预期值的状态。 基于这些批评，演员不断更新其策略。

​      演员—评论家算法的两个显著特点是认为大脑可能会实现这样的算法。首先, 演员和评论家算法的两个组成部分—演员和评论家—暗示纹状体的两个部分 (背侧和腹侧的细分; 见15.4 节), 对于以奖励为基础的学习都至关重要，可能像演员和评论家一样运作。表示大脑实现演员—评论家算法的第二个属性是，$TD​$错误具有作为演员和评论家的强化信号的双重作用，尽管它们对于每个这些组件的学习有不同的影响。这适用于神经电路的几个性质：多巴胺神经元的轴突靶向纹状体的背侧和腹侧细分; 多巴胺似乎对调节两种结构的突触可塑性至关重要; 以及神经调节剂如多巴胺作用于靶结构的方式取决于靶结构的性质，而不仅仅取决于神经调节剂的性质。

​      第13.5节提出了作为策略梯度方法的演员—评论家算法，但是$Barto，Sutton和Anderson（1983）$的演员—评论家的算法比较简单，采用人工神经网络的形式。 （参见第9.6节关于人造神经网络的基础知识）在这里我们描述一个类似于$Barto$等人的神经网络实现，并且我们遵循$Takahashi，Schoenbaum和Niv（2008）$给出了一个关于这个人造神经网络建议，这个神经网络如何在大脑中由真实的神经网络实现。 我们推迟对演员—批评者学习规则的讨论，直到以下部分（第15.8节），我们将其作为策略梯度制定的特殊情况，并讨论他们对多巴胺如何调节突触可塑性的建议。

​      图$15.6a​$ 给出了一个以演员—评论家为组件的人工神经网络的实施方法。评论家由一个单一的神经元样单元，$V​$ ， 其输出活动表示状态值。和一个组件显示为菱形标记 $TD​$, 通过将 $V​$ 的输出与奖励信号和以前的状态值组合在一起来计算 $TD​$误差 (如由 $TD​$ 菱形到自身的循环所建议)。演员网络具单层在$k​$标记为$Ai,i = 1,…,k​$个单元中。每个动作单元的输出是$k​$维动作向量的分量。（另一种选择是有$k​$个单独的动作，一个由每个演员单位只会着，相互竞争着执行，但是在这里我们将把整个$A​$向量看作一个动作）评论家和演员网络都接收由代表智能体环境状态的多个功能的输入。该图显示了这些功能的圆圈标记$\oslash_1,\oslash_2,…,\oslash_n​$ (显示两次只是为了保持图形简单)。

---

一个权重代表了突触的功效与每个连接从状态特征$\oslash _i$到评论家单位$V$和对每个单位动作$A_i$ 。评论家网络中的权重是值函数参数化, 演员网络中的权重是策略参数化。 网络学习通过评论家和演员的学习规则而导致的权重变化，我们在下面的部分描述。

​      在评论家和演员网络中, 电路产生的 $TD$误差是改变权重的增强信号。这在图$15.6a$中被标记为 "$TD$误差$\delta$" 的线在评论家和演员网络的所有连接之间延伸。这方面的网络实现， 连同预测奖励错误假说和 多巴胺神经元的活动是如此广泛分布在这些神经元的广泛轴突乔木事实， 表明， 一个演员—评论家网络的东西可能不会太牵强, 作为一个假说来说明在大脑中是如何进行奖励相关的学习。

![Figure 15.6](D:\trans\rl-intro-book-chinese\chapter15\img\Figure 15.6.png)

​                                                    图15.6：演员—评论家人工神经网络和假设神经实现。

 a）作为人工神经网络的演员批评者算法。演员根据评论家收到的$TD$误差$\delta$调整政策;评论家使用相同的$\delta$来调整状态值参数。评论家从奖励信号$R$和当前状态值估计值的变化中产生了$TD$误差。演员不能直接获得奖励信号，评论家不能直接访问该动作。 b）演员—批评者算法的假设神经实现。评论家的演员和价值学习部分分别放置在纹状体的腹侧和背部部分。 $TD$误差由位于$VTA$和$SNpc$的多巴胺神经元传递，以调节从皮质区域到腹侧和背侧纹状体的输入突触功能的变化。改编自神经科学的前沿，vol.2（1),2008,Y. Takahashi，G.Schoenbaum和Y. Niv，沉默评论家：在$Actor/Critic$模型的背景下了解可卡因敏化对背外侧和腹侧纹状体的影响。

---

​     图$15.6b$显示 - 非常有代表性 - 根据$Takahashi$等人的假设，图左侧的人造神经网络如何映射到大脑中的结构上（2008年）。 这个假设把评论家的角色和价值学习部分分别放在纹状体的背部和腹部，即基底神经节的输入结构。 回顾第15.4节，背根神经症主要涉及行为选择，腹侧纹状体被认为对奖励处理的不同方面至关重要，包括情感价值与感觉的分配。 大脑皮层与其他结构一起向纹状体输入信息，传达有关刺激，内部状态和运动活动的信息。

​    在假设的演员-评论家大脑实施中，腹侧纹状体向$VTA$和$SNpc$发送价值信息，其中这些核酸中的多巴胺神经元具有关于奖励的信息以产生对应于$TD$误差的活动。 图$15.6a$中的“$TD$误差$δ$”线在图$15.6b$中成为标记为“多巴胺”的线，其代表$VTA$和$SNpc$中多巴胺神经元的广泛分支轴突。 参考图$15.2$，这些轴突与脊髓上的棘突突触接触，即中等多刺神经元的突起，纹状体的背部和腹部分裂的主要输入/输出神经元。 将输入到纹状体的皮质神经元的轴突在这些刺的尖端上进行突触接触。 根据这个假设，在这些棘突中，从皮质区到层的突触的功效的变化受到严格依赖多巴胺提供的加强信号的学习规则的约束。

​      图$15.6b$所示假设的一个重要含义是多巴胺信号不是像增强学习的标量$R_t$那样的“主”奖励信号。事实上，假设意味着人们不一定能够探测大脑，和记录任何单个神经元活动中的$R_t$信号。许多相互关联的神经系统产生奖励相关信息，根据不同类型的奖励采用不同的结构。多巴胺神经元从多个不同的脑区域接收信息，因此图$15.6b$中标记为“奖励”的$SNPc$和$VTA$的输入应该被认为是沿着多个输入通道到达这些核中的神经元的奖励相关信息的向量。那么理论标量奖励信号$R_t$可能对应于所有奖励相关信息对多巴胺神经元活动的净贡献。这是大脑不同区域的多个神经元的活动模式的结果。

​      虽然图$15.6b$所示的演员-评论家神经实现在某些方面可能是正确的，但它显然需要被改进，扩展和修改，以符合多巴胺神经元相位活动功能的全面模型。 本章末尾的“历史和书目备注”部分引用了更多详细讨论这一假设的实证支持和不足之处的出版物。 我们现在详细介绍一下演员和评论家的学习算法对于管理皮质神经节突触突触功能变化的规则。

#### 15.8演员和评论家学习规则

​      如果大脑确实能实现像演员—批评者算法—并且假设多巴胺神经元的群体向背面和腹侧纹状体的皮质纹状体突触广播共同的加强信号，如图$15.6b$所示（这是我们上面提到的过度简化)—此信号以不同的方式影响这两个结构的突触。评论家和演员的学习规则使用相同的加强信号，$TD$误差$\delta$，但其对学习的影响对于这两个组件是不同的。结合资格跟踪，$TD$误差告诉演员如何更新动作概率以达到更高价值的状态。演员的学习就像使用效应法学习规则的工具条件（第1.7节）：演员的作用是尽可能保持$\delta$。另一方面，$TD$误差与资格追踪相结合，告诉评论家改变价值函数参数的方向和数量，以提高其预测精度。评论家致力于使用诸如经典条件的$TD$模型（第$14.3$节）的学习规则，将$\delta$的幅度尽可能接近零。评论家和演员学习规则之间的区别比较简单，但这种差异对学习有深远的影响，对于演员—评论家的算法如何运作至关重要。差异仅在于每种学习规则使用的资格追踪。

​    下面的框将基于方框13.5的伪代码重新描述了一个演员—评论家算法的策略梯度公式。

![Policy-Gradient](D:\trans\rl-intro-book-chinese\chapter15\img\Policy-Gradient.png)

   通过以某种方式参数化策略和价值功能，演员和评论家可以通过人工神经网络实现，如图$15.6a$所示。首先, 值函数的参数化是简单的线性参数:

$\widehat{v}(s,W)=W^{T}\oslash(s)$ ，

其中$\oslash(s)$是状态$s$的特征向量表示。 然后，我们可以将值函数作为单个线性神经元样单元的输出，称为评估单元，并在图$15.6a$中标为$V$。

---

​     评估单元计算由$w_j,j = 1,…,m.$加权的输入和$\oslash_j(s)$。 每个$\oslash_j(s)$就像对神经元突触的突触前信号，其功效是$w_j$。 权重根据上面的规则进行更新：$w\leftarrow w + \beta\delta e^w$，其中加强信号$\delta$对应于广播给所有批评者单位的突触的多巴胺信号。  

​      评论单位的资格追踪矢量$e^w$是一个痕迹

​               $\nabla_w \widehat{v}(s,w)=\oslash(s)$                                                                                                                          (15.1)    

对于过去的状态$s$。在神经术语中，每个突触都有自己的资格痕迹，它是矢量$e^w$的一个组成部分。 突触的资格迹象不断衰减，但根据到达突触的活动水平累积，即突触前活动的水平，此处由到达该突触的特征向量$\oslash(s)$的组成部分表示。 只要突触的资格跟踪不为零，我们就说突触有资格进行修改。 突触的效力实际上如何改变取决于突触合格时到达的强化信号。 我们称这些批评单位突击非偶然资格痕迹的资格痕迹，因为它们只依赖于突触前的活动，并不以任何方式突发突触后的活动。

​     批评家单位的突触的非或有资格痕迹意味着批评家的学习规则本质上是第14.3节描述的经典条件的$TD$模型，这是产生平行多巴胺神经元活动的$TD$误差的相同规则。 根据我们上面给出的评论单位及其学习规则的定义，图$15.6a$的评论者与巴托等人神经网络演员评论家的评论家一样（1983年）。 显然，像这样一个像线性神经元一样的单元组成的评论家是最简单的起点；这个评论单位是一个更复杂，甚至深刻的神经网络的智能体，能够学习更复杂的价值功能。

​      图$15.6a$中的演员是$k$个神经元状的演员单元的单层网络，其中每个的输入都是评论单元收到的相同的特征向量$\oslash(s)$。 这些单位在上面框中遵循策略梯度公式的一个方法是每个都是具有**强化**策略梯度学习规则的伯努利后勤单位。 参考练习13.7，它定义了这种单元，并要求你给出它的**强化**学习规则。 每个演员单位的输出$i,i = 1,...,k$是具有值0或1的随机变量$A_i$。考虑值1作为神经元发射，即发射动作电位。 单位$i$输入特征向量的加权和$\theta^T_i\oslash(s)$确定其通过逻辑函数的动作概率：

​    $\pi(1|s,\theta_i)=1-\pi(0|s,\theta_i)=1/(1+\exp(-\theta^T_i\oslash(s)))$

 每个演员单元i的权重通过上面方框中的规则进行更新：$\theta_i \leftarrow \theta_i+ \beta\delta e^\theta_i$，其中$\delta$再次对应于多巴胺信号。 这与所有评论单位的突触发出的加强信号相同，图$15.6a$显示它被广播到所有演员单位的所有突触（这使得这个演员网络成为强化学习智能体团队，我们 在下面的$15.10$节讨论）。

---

练习$13.7$要求你表达$\nabla_\theta\log pi(A_t|S_t,\theta_t)$，通过计算梯度，以$A_t$ ，$\oslash(S_t)$和$\pi(A_t|S_t,\theta_t)$为单位累积在伯努利逻辑单位的**强化**资格向量中的数量。 我们正在寻找的答案是：

$\nabla_{\theta_i} \pi(A_i|S,\theta_i)=(A_i -\pi_i(A_i|S,\theta_i))\oslash(S)$                                                                                            (15.2)

​    与$(15.1)$给出的仅仅累积突触前活动的评论家突触的非或有资格迹象不同，演员单位的突触的资格跟踪还取决于演员单位本身的活动。我们认为这是偶然的资格追踪，因为它取决于这个突触后的活动。每个突触的资格痕迹不断衰减，但依赖于突触前神经元的活动而增加或减少，以及突触后神经元是否发射。 $(15.2)$中的因子$A_i-\pi_i(A_i|S,\theta_i)$在$A_i = 1$时为正，否则为负。演员单位合格迹象中的突发性事件是评论家和演员学习规则的唯一区别。通过记录在不同状态下采取不同行动的信息，或有资格的追踪允许奖励（积极的$\delta$）或惩罚的责任（否定的$\delta$）被分配在策略参数之中，这里是一个演员单位的突触的功效，根据这些参数的值$\delta$对单位产量的贡献贡献于后期价值的贡献。或有资格痕迹标记突击，如何修改它们以改变单位的反应，以支持积极的$\delta$。

​      评论家和演员学习规则对于皮质神经节突触的功效如何改变提出了什么建议？ 两种学习规则都与唐纳德·希伯（$Donald Hebb$）的经典建议相关，即每当突触前信号参与激活突触后神经元时，突触的功效就会增加$（Hebb，1949）$。 评论家和演员学习规则与$Hebb$的提案一致，即突触功效的变化取决于几个因素的相互作用。 在评论学习规则中，相互作用是仅依赖于突触前信号的加强信号$\delta$和资格迹线之间的相互作用。 神经科学家称之为双因素学习规则，因为相互作用是在两个信号或数量之间。 另一方面，演员学习规则是一个三因素学习规则，因为除了依赖于$\delta$以外，其资格追踪依赖于突触前和突触后活动。

​     与$Hebb$提案的最接近的联系是演员单位突击的或有资格痕迹的性质。 正如$Hebb$提出的那样，如果突触前信号参与激活突触后神经元，而不是突触的功效增加，基本上这个相同的条件增加了一个行为者单位的突触的资格痕迹。 突触的功效如何变化取决于所接收的加强信号，而其资格跟踪不为零，即突触有资格进行修改。 换句话说，而与$Hebb$的提议之前和突触后的重合活动累积在突触的功效，这里它积累在突触的资格追踪。

---

​    演员的随时资格痕迹具有关键性质，可能并不明显。 在定义这些资格痕迹的表达式$(A_i-\pi_i(A_i|S,\theta_i)\oslash(S)$中，突触后因子$A_i-\pi_i(Ai|S,\theta_i)$是突触前因子$\oslash(S)$的函数; 就像$Hebb$的提议一样，突触前的活动$\oslash(s)$参与引起$A_i-\pi_i(A_i|S,\theta_i)$中出现的突触后活动。 这对于为以后到达的任何强化信号正确分配信用是至关重要的。 参与制作智能体的任何行动的突击，之后加强，值得加强的一些信用。

​    现在，由于我们已经在图$15.6$的评论家和演员网络中定义了神经元样单元，所以输入向量到达单元之间的距离和单元响应于该输入的活动之间没有延迟。这是许多抽象模型的$Hebbian$型可塑性的属性，忽略从突触前神经元输入的影响突触后神经元的射击活动的时间。在这些模型中，突触功效根据同时前和突触后活动的简单产物而改变。实际上，突触输入可以使神经元激活几十毫秒。当突触前神经元的动作电位到达突触时，神经递质分子被释放，其必须扩散穿过突触间隙到突触后神经元并结合突触后神经元表面上的受体，以激活引起突触后神经元的分子机制开火。这个激活时间必须在更现实的$Hebbian$型可塑性模型中考虑，并且还要形成演员型单位的或有资格痕迹。要正确分配信用，定义资格跟踪的突触前因素必须是也定义了跟踪的突触后因子的原因。更实际的演员单位将考虑这个激活时间。 （这个激活时间不能与神经元接收受神经元活动影响的加强信号所需的时间混淆，资格跟踪的功能是跨越这一般更长的时间间隔，我们将在下面的部分进一步讨论。）

​     神经科学家已经发现了一种称为穗定时可塑性（$SEDP​$）的希伯比可塑性形式，使得在脑中存在类似突触可塑性的似是而非的。 $STDP​$是可比性的$Hebbian​$型，但突触功能的变化取决于突触前动作电位（即，引起突触释放神经递质的动作电位或尖峰）的相对时间和动作电位 突触后神经元依赖可以采取不同的形式，但是在最受研究的一个方面，如果通过突触传入的尖峰在突触后神经元发生之前到达，则突触的强度会增加。 如果时序关系相反，在突触后神经元发生后不久就会出现突触前尖峰，则突触的强度降低。 $STDP​$是一种$Hebbian​$可塑性，它考虑到神经元的激活时间，这是类似演员的学习所需要的成分之一。

​      类似演员学习也需要神经调节因子的作用，如多巴胺。 $STDP$的发现已导致神经科学家调查三因素形式的$STDP$的可能性，其中神经调节输入必须遵循适当的定时的突触前和突触后尖峰。这种形式的突触可塑性，称为奖励调制的$STDP$，非常类似于这里讨论的演员学习规则。由常规$STDP$产生的突触变化仅在进入尖峰紧随其后的突触后尖峰之后在时间窗内存在神经调节输入时发生。证据正在积累，奖励调制的$STDP$发生在背纹纹素的中等多刺神经元的脊柱上，多巴胺提供神经调节因子 - 在图$15.6$所示的演员评论者算法的假设神经实现中发生角色学习的部位湾实验已经证明了奖励调制的$STDP$，其中仅在神经调节脉冲到达时间窗口内才能持续长达10秒的情况下才发生皮质脊髓束突触功能的持续变化，突触前穗紧随其后突触后穗（$Yagishita$等。2014）。虽然这些证据是间接的，但这些实验表明存在具有延长课程的或有资格的痕迹。产生这些痕迹的分子机制以及可能低于$STDP$的更短的痕迹尚未被理解，但是关注时间依赖性和神经调节剂依赖性突触可塑性的研究仍在继续。

​       我们在这里描述的神经元样的演员单位，其效应法学习规则，在$Barto$等人的演员—评论家网络中出现了一些简单的形式（1983年）。 该网络受到生理学家$A.H.Klopf(1972,1982)$提出的“享乐主义神经元”假说的启发。 不是所有的$Klopf$假设的细节都与突触可塑性有关，而是发现$STDP$，以及对$STDP$的回调调制形式越来越多的证据表明，$Klopf$的想法可能不会太离谱。 接下来讨论$Klopf$的享乐主义神经元假说。

​    

​     

​       









​       



