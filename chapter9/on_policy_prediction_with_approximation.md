## 第9章

# 函数拟合与在策（On-policy）预测

​	本章，我们将开始探究如何在强化学习中用函数拟合（function approximation）从在策数据中估计状态值函数。即，从已知策略$\pi$产生的经验中去估计$v_\pi$。本章的亮点在于用于估值的值函数不再是一张表，而是由权重向量$\theta\in\mathbb{R}$来表示的一个参数化的函数，记$\hat{v}(s,\theta)\approx v_\pi(s)$为给定权重向量$\theta$下状态$s$的估值。例如，$\hat{v}$可以是一个用特征向量$\theta$来表示状态的线性函数。更一般地，$\hat{v}$可以是个由多个隐含层组成 的人工神经网络来计算的函数，其中$\theta$向量作为连接各层权重的参数。通过调整权重，任何方程都可通过神经网络来模拟。又或者$\hat{v}$可以是个决策树函数，其中$\theta$代表决策中分流点和叶节点的值。一般来说，权重的数量（$\theta$集合中的元素数）要远小于状态数（$n<s\ll|S|$），一个权重的改变会影响很多状态的估值。(这种估值方法的)结果是9，当仅更新了一个状态后，因为该状态而衍生的变化会影响很多其他状态的估值。这种泛化特性潜移默化地增强了估值的学习效果，同时也变得让人更加难以处理和理解。



### 9.1 值函数(Value-function)估计

​	本书所言的所有预测方法都被描述为备份（backups）。换言之，备份就相当于更新---通过改变某状态的当前估值，使其趋向于该状态的”备份估值“（backed-up value）来完成状态函数的估值更新。记某个元素的备份为$s\to g$ ，其中$s$代表被备份的状态，而$g$代表被备份的值，即$s$更新后的目标值。例如，蒙特卡洛法中估值备份为$S_t\to G_t$，$\rm TD(0)$法中的备份为$S_t\to R_{t+1}+\gamma\hat v(S_{t+1},\theta_t)$，而$n$步$\rm TD$备份法中的备份为$S_t\to G_t^{(n)}$。在$\rm DP$（动态规划/dynamic programming）中策略估值备份则表示为$s\to \mathbb{E}_\pi[R_{t+1}+\gamma\hat v(S_{t+1},\theta_t)|S_t=s]$，其备份了某个$s$。另外其在实际任务过程中，备份的是当前任务中遇到的$S_t$。

​	很自然地，我们将借助每个备份（backup）以寻找估值函数中输入输出间的行为模式。 某种意义上，备份$s\to g$意味着$s$的估值应该趋向于$g$才对。 到目前为止，实际执行的备份都是很容易理解的：表格法中的$s$估值在一定程度上向$g$的方向更新了一点，同事其他的状态估值并没有因此发生变化。 而现在，我们允许使用些更复杂且难以理解的方法来操作备份。这类方法下对$s$的估值更新也会随之影响到其他状态的估值。 机器学习中，用以模拟样本中输入-输出规律的方法被称之为“监督学习”。而当输出是数字（比如$g$）时，整个（模拟）过程往往被称为“函数拟合”（function approximation）。 函数拟合通过接收具有期望行为（规律）的输入-输出样本来进行后续的估值。所以，只要将每个备份$s\to g$作为训练样本并传递给用以拟合的函数。经过拟合后，就可将拟合后的估值函数用于状态的估值了。

​	将每份备份视作一般的训练样本使得我们可以使用各种现存的函数拟合法来进行估值预测。基本上我们可以使用任何基于样本的监督学习方法，包括人工神经网络、决策树、以及多种多样的多变量回归（multivariate regression）。然而，并非所有函数拟合法都能很好地适用于强化学习。最玄妙的神经网络和统计法都需要先假设有一个静态的训练集，即在该假设下默认是有多份样本被同时传入用于估值函数中的。而在强化学习中要在和环境（或其模型）交互中“在线”地学习是很有必要的。而要做到这点，需要确保学习方法能通过逐步获得的数据来高效地进行学习。另外，适用于强化学习的函数拟合法一般要能处理非静态的目标函数（随时间而变的目标函数）。例如，在使用基于$\rm GPI$（generalized policy iteration）的控制方法时，我们往往需要在$\pi$变动的情况下学习$q_\pi$。即使策略表示一直不变，由bootstrapping法（$\rm DP$和$\rm TD$学习法）产生的训练样本目标值也是非静态的（译者：比如环境本身发生了改变，又或者给定的输出本身就不是正确的估值。那么自然所期望的输入输出样本是非静态的，即对某个x，期望的y是可变的，非静态的）。所以无法有效处理非静态数据的方法就不适用于强化学习了。



### 9.2 预测指标（均方误差/MSVE）

​	目前为止我们尚未明确指出用于预测的指标。因为学习到的估值函数完全可能等价于真正的估值函数，所以在函数拟合法中不需要像表格法那样不断地对预测质量进行评估。而且，（表格法中）学习到的状态估值是彼此分离的---即对任意状态的估值更新不会影响到影响到其他状态的估值。 而估计法中，对某状态的更新会影响到许多的状态，因此无法保证所有状态估值的绝对正确。假设任务中状态个数大于权重个数，则某状态的估值越是精确，其他状态估值的误差就会越大。那么在这种情况下，我们有义务说明哪些状态我们更加关心。我们必须确定一个权重（weighting）或分布（distribution）$d(s)\geq 0$来表示对各个状态$s$误差的关心程度。 这里指的误差（error）是估值$\hat v(s,\theta)$和真值$v_\pi(s)$的差的平方。通过分布$d$在状态空间中对误差进行衡量，于是很自然地我们得到了一个目标函数---“均方误差”（ the Mean Squared Value Error)或者称为MSVE。


$$
MSVE(\theta)\doteq \underset {s\in \mathbb{S}}\sum d(s)[v_\pi(s)-\hat v(s,\theta)]^2 \tag{9.1}
$$
对均方误差开方，即得均方误差根---RMSVE,其给出估值偏离真值程度的一个大概估计，该值经常被用于作图。关于$d(s)$的选择，一个典型的做法是取策略$\pi$下用于状态$s$的时间占比。 我们称其为“在策分布”（ on-policy distribution）；本章我们将着重全面地分析该分布下的情况。另外在连续任务中，在策分布就是$\pi$下的静态分布。

> ---
>
> 回合制任务中的在策分布
>
> ---
>
> ​	在一个情景任务中，在策分布略有不同，原因是它取决于每个回合所选择的初始状态。记$h(s)$为每回合开始于状态s的概率。同时记$\eta(s)$为平均每个回合$s$所步过（experience）的总时步。当每个回合开始于状态$s$，又或者从已步过的某个状态$\bar s$转移到状态$s$时，我们就说状态$s$已步过：
>
>
> $$
> \eta(s)=h(s)+\underset{\bar s}\sum \eta(\bar s)\underset{a}\sum \pi(a|\bar s)p(s|\bar{s},a), \forall s\in \mathbb{S}. \tag{9.2}
> $$
> 通过访问$\eta(s)$的期望值可以解决该方程，而通过正则化$\eta(s)$使之和为1则可得在策分布：
> $$
> d(s)={\eta(s)\over\sum_{s'}\eta(s')}.  \tag{9.3}
> $$
>

---

尽管持续性任务和回合制任务的行为较类似，但在本书的这一部分，正如你将反复看到的那样---在函数拟合法的正式分析中，我们将区别对待这两种情况。而这种（分类）也规范了学习目标。

​	在强化学习中，我们尚不完全清楚$\rm MSVE$是否就是正确的性能衡量指标。不要忘了之所以需要学习估值函数是为了通过其找到一个更好的策略，这才是终极目标。所以基于该目标，最好的估值函数未必需要最小化的$\rm MSVE$。然而，在目前尚不清楚是否有更好的选择下，我们将专注于$\rm MSVE$作为衡量性能的指标。

​	基于$\rm MSVE$，理想目标是找到一个“全局最优解”（global optimum），即对所有$\theta^*$和$\theta$，有$\rm MSVE(\theta^*)\leq \rm MSVE(\theta)$。对于一些诸如线性方程之类的简单函数，这个目标是可以做到的。但对于诸如人工神经网络和决策树这类复杂的函数模拟器则不太可能。这类复杂函数模拟器的缺点是---相较于全局最优，其最后可能会收敛至“局部最优”---即在$\theta^*$的某个周边区域，有$\rm MSVE(\theta^*)\leq \rm MSVE(\theta)$。尽管“局部最优”不是很让人放心，但对于非线性函数模拟器，这往往已是最好的，且足够了。但在许多强化学习的案例中，我们依然无法保证收敛至最优，哪怕是收敛至最优解的某个固定邻域内。 实际上，某些方法（反而）可能会发散---即在极限中他们的$\rm MSVE$趋向于无穷。

​	在前2章中，通过前序备份集产生供后续使用的训练样本，我们讨论了大量强化学习方法并将其用于估值预测；若在此基础上再结合大量的函数拟合法，我们就构造了一个（完整 的）强化学习框架。 为了最小化（估值）偏差，我们也引入了$\rm MSVE$性能指标。 可用的函数拟合法实在太多了，而且对于大部分方法，我们也对如何给出其可靠的估计或推荐知之甚少。所以出于必要性，我们仅考虑有限的几个方法。在本章后续部分，我们将专注于基于梯度的函数拟合法，尤其是线性梯度下降法。 之所以重点讨论这些方法是因为我们认为他们是最有前景的方法；也因为这些方法揭示了关键原理；当然也因为我们篇幅有限，而这些方法也足够简单。



### 9.3 随机梯度法和半梯度法

​	在众多用以估值预测的学习方法中，现在我们开始深入探究基于随机梯度下降（$\rm SGD$）的函数拟合方法。应该说在所有函数拟合法中，$\rm SGD$是最为广泛使用的方法，并且其很适用于在线的强化学习。

​	梯度下降法中的权重向量是一个由固定数量的元素$\theta\doteq(\theta_1,\theta_2,...,\theta_n)^T$[^1]组成的列向量，而估值函数$\hat v(s,\theta)$是一个关于定义域在$s\in \mathbb{S}$的$\theta$的可微函数。由于在每个一系列的离散时步$t=0,1,2,3,...,$中$\theta$会得到更新，所以我们需要定义$\theta_t$为每一时步中的权重向量。现在假设，每一时步中，我们观察到了一个新的样本$S_t\to v_\pi(S_t)$，包括（可能是随机选择的）状态$S_t$和其在该策略下的真值。在与环境互动中，这些状态有可能是连续的，不过在此我们先不做该假设。即便针对每一个$S_t$能给出一个绝对正确的值$v_\pi(S_t)$，但因为资源的受限，我们的函数拟合器的精度也会有所限制。函数拟合不是件简单的事情。尤其是在多数情况下，没有$\theta$能保证在所有状态甚至是所有样本中其估值的绝对正确。另外，$\theta$需要能对从没在样本中出现的状态进行(有效的)泛化。

[^1]: 符号$T$表示转置，在这里代表把一个行向量转置为列向量；本书中普遍采用列向量作为默认向量，除非显式地写成行向量或配以转置符号（比如本例）。

​	假设所有样本中的状态均服从同一分布$d$，在该假设下我们试图最小化（9.1）中给出的$\rm MSVE$。一个好主意是最小化那些已经观察到的样本的误差。“随机梯度下降法”（stochastic gradient-descent/SGD）在步过每一份样本后，通过将权重向量往能最小化该样本误差的方向挪动一小步来更新权重：
$$
\begin{eqnarray}
\theta_{t+1}&\doteq&\theta_t-{1\over2}\alpha\nabla[v_\pi(S_t)-\hat v(S_t,\theta_t)]^2 \tag{9.4}\\
                         &=&\theta_t+\alpha[v_\pi(S_t)-\hat v(S_t,\theta_t)]\nabla\hat v(S_t,\theta_t), \tag{9.5}
 \end{eqnarray}
$$


式中$\alpha$是个取正值的步长参数，而对于任意标量表达式$f(\theta)$，$\nabla f(\theta)$代表权重向量中所有元素的偏导数组成的向量：
$$
\nabla f(\theta)\doteq({\partial f(\theta)\over\partial\theta_1},{\partial f(\theta)\over\partial\theta_2},...,{\partial f(\theta)\over\partial\theta_n})))^T. \tag{9.6}
$$
该导数向量是$f$关于$\theta$的梯度。SGD之所以被称为“梯度下降”是因为在所有时步中，$\theta_t$正比例于样本方差的负梯度（9.4）。在这样的方向下误差可以降低得最快。而称其为“随机”梯度法是由于仅有一个（随机抽取的）样本会被用来更新$\theta$。$\theta$每次都挪动一小步,那么在步过了大量样本后，最后就会达到最小化如$\rm MSVE$这样的平均性能指标的效果。

​	我们可能无法理解为何每次$\rm SGD$仅往梯度方向挪动一小步？为何不忽略某样本中的所有误差，进而直接往该方向更新全部的梯度呢？原因是很多时候这样做是可行的，但往往不推荐。 记住我们并不需要一个误差为0的估值函数，而只需要该函数可以权衡所有不同状态的误差。若在某一时步中我们调整权重使其对该时步的状态估值绝对正确，则我们将无法找到全局平衡。事实上，$\rm SGD$的最终收敛需要假设$\alpha$在不断地递减。若$\alpha$如标准随机估计条件（2.7）那样递减，则$\rm SGD$法(9.5)能够保证函数最终收敛至局部最优。

​	现在来看这样一个例子。该例中$U_t\in \mathbb R$作为第$t$个训练样本的目标输出，$S_t \to U_t$，其不是真值$v_\pi(S_t)$，而可能是某个随机生成的模拟值。比如，$U_t$可以是个有噪声的$v_\pi(S_t)$；又或者如前述章节提到的那样，是bootstrapping法中的其中一个$\hat v$。在这种情况下因为$v_\pi(S_t)$是未知的，所以无法直接使用式（9.5）来更新梯度。但可以用$U_t$来代替$v_\pi(S_t)$。 这个想法引申出了如下用于状态估值预测的一般SGD方法：

$$
\theta_{t+1}\doteq\theta_t+\alpha[U_t-\hat v(S_t,\theta_t)]\nabla \hat v(S_t,\theta_t). \tag{9.7}
$$
---

> 用蒙特卡洛梯度法估计$\hat v \approx v_\pi$
>
> ---
>
> 输入：用于评估的策略$\pi$
>
> 输入：一个可微函数$\hat v:\mathcal S \times \mathbb R^n\to\mathbb R$
>
> 对估值函数的权重$\theta$做合适的初始化（例如 设 $\theta=0$）
>
> 一直重复：
>
> ​	用$\pi$生成一个包含$S_0,A_0,R_1,S_1,A_1,...,R_T,S_T$的回合
>
> ​	对$t=0,1,...,T-1:$
>
> ​		$\theta \leftarrow \theta+\alpha[G_t-\hat v(S_t,\theta)]\nabla\hat v(S_t,\theta)$

---

在一般随机近似中，若$U_t$是无偏估计，即对任意$t$，若有$\mathbb E[U_t]=v_\pi(S_t)$，则在满足了2.7式中$\alpha$的递减条件下，$\theta_t$就能保证收敛至局部最优。

​	例如，若样本中的状态是在策略$\pi$下通过与环境（模拟）交互中产生的，则因为状态的真值就是该状态的期望回报，所以根据对$v_{\pi}(S_t)$的无偏估计定义，有蒙特卡洛目标$U_t\doteq G_t$。而有了这个选择后，就可保证在一般$\rm SGD$方法（9.7）中对$v_\pi(S_t)$的估值是可收敛至局部最优的了。因此，梯度下降版中的蒙特卡洛估值法是可以保证找到一个局部最优解的。相应的伪代码已展示在上图框中。

​      注意，若目标$U_t$是由$v_\pi(S_t)$的bootstrapping估计来表示的，则得不到收敛保证。诸如$n$步$G^{(n)}_t$或者$\rm DP$目标中的$\sum_{a,s',r}\pi(a|S_t)p(s',r|St,a)[r+\gamma \hat v(s',\theta_t)]$的Bootstrapping目标值均依赖当前的权重向量$\theta_t$。其暗示了这种梯度下降是带有偏见的，所以基于这类目标值是无法不能执行一个真正意义上的梯度下降法的。 （对于该结论的）一个解释可以看式（9.4） 到（9.5）这一关键步骤转化。其需要保证目标值是独立于$\theta_t$的。所以当用bootstrapping法代替了原式中的$v_\pi(S_t)$，这一转化将不再成立。可见bootstrapping法实际上不是梯度下降的真正实例（Barnard,1993）。这种方法考虑了$\theta_t$的变化对估值部分的影响，却有意忽略了其对目标值的影响。所以此类方法只包含了部分梯度。鉴于此，我们称其为半梯度法（ semi-gradient methods）。

​	虽然半梯度（bootstrapping）法无法像一般梯度法那样稳定收敛。但正如下节将讲述的那样，在诸如使用线性函数这类重要的估值函数的情况中其确实可保证收敛。而且这类方法也确实因为一些优势而受到偏爱。优势一是这类方法往往学习得非常快，正如6、7章所述。其二是它们可直接在线地持续不断地学习，而不用等到回合结束才开始学习。而这个特性使得该方法适用于连续型问题并借此发挥其计算优势。一个使用半梯度法的范例是半梯度$\rm TD(0)$法，其目标值为$U_t\doteq R_{t+1}+\gamma\hat v(S_{t+1},\theta)$。下页的顶框中展示了该方法的完整伪代码。

---

> $用半梯度TD(0)法来估计\hat v\approx v_\pi$
>
> ---
>
> 输入：用于评估的策略$\pi$
>
> 输入：可微函数$\hat v:\mathcal S^{+}\times \mathbb R^{n}$，其中$\hat v(最终状态, . )=0$
>
> 初始化估值函数的权重$\theta$,可取任意值（比如：$\theta =0$）
>
> （每个回合）重复：
>
> ​	初始化$S$
>
> ​	（回合中的每个时步）重复：
>
> ​		选择$A\thicksim\pi(.|S)$
>
> ​		实施动作$A$，观察$R,S'$
>
> ​		$\theta\leftarrow\theta+\alpha[R+\gamma\hat v(S',\theta)-\hat v(S,\theta)]\nabla\hat v(S,\theta)$
>
> ​		$S\leftarrow S'$
>
> ​	直到$S'$为终极状态		
>
> ---



**示例 9.1：从一个由1000个状态组成的随机步行任务中整合状态**	“状态整合”是一般函数拟合的一个简单形式。其将状态集进行分组，用一个值（权重向量$\theta$中的一个元素）来表示每一组状态的估值。某一状态的估值就是其所属组别元素的值。当该状态值被更新时，仅代表该组的元素（相较于其他权重向量元素）会被更新。状态整合是$\rm SGD$ (9.7)的一个特殊情况，其中梯度，$\nabla \hat v(S_t,\theta_t)$为1代表状态$S_t$组的元素，为0代表其他组别的元素。

​	考虑一个有着1000个不同状态的随机步行任务（示例6.2和7.1）。所有状态从左至右被标示为1~1000。所有回合初始化于中心点附近，即状态500。任务中智能体会从当前状态等可能地转移至其左右相邻的100个状态中的某一个状态。当然，如果当前状态接近某测边缘，则可转移至该侧的状态集可能会少于100个。这种情况下，那些丢失的状态概率被转移到了该侧的终止状态（因此，状态1有0.5的概率在其左侧终止，而状态950有0.25的概率在其右侧终止）。按照惯例，我们设左侧终止状态的回馈为-1，而右侧的回馈为+1。其他所有状态的转移奖励为0。本案例将作为滚动示例反复出现在本节讨论中。

​	图9.1展示了该任务的真值函数$v_\pi$。可见其几乎是一条直线。并且其在水平方向上稍稍斜了一点，尤其在两侧的最后100个状态中弯曲得更加厉害。图中也显示了由$\alpha=2\times10^{-5}$的学习率结合状态整合的蒙特卡罗算法，在学习了100,000个回合得到的最终状态估值函数。关于状态整合，本例中是将1000个状态分成10组，每组100个状态（举例：状态1-100在组1，状态101-200在另一组，以此类推）。可见，状态整合的一个典型结果就是图中所示的阶梯效果；其中每一分组中的估值都是恒等的，相邻组别的估值有很明显的落差。这些估值接近于$\rm MSVE$（9.1）中的全局最小值。

![](img/Figure 9.1.png)

​	$图 9.1：在1000状态随机步行任务中使用蒙特卡洛算法的函数拟合$

​	要观察估值的一些细节，最好的做法是观察状态分布$d$，即上图下半区域，其刻度在图右侧。正中央的状态500是回合的初始状态，其在一个回合中很难被再次访问到。平均来说1.37%的时步是在初始状态。而从初始状态走一步就能到达的状态是第二类最常被访问到的状态，可见有0.17%的时步是在这些状态中。之后所有的状态分布$d$近乎线性地降低，直至最边缘处状态1和状态1000的0.0147%。这种分布最直观的效果就是图中最左侧分组中的状态估值明显要比无权重的平均真值要高一点；反之，右侧的状态估值相应的比平均真值要低一点。该现象是由于在$d$分布下，这些组的状态权重有极大的不对称性。比如在最左侧组中，状态99的权重要比状态0高出3倍。因此相较于状态0，该组的估值会偏向于比其高的状态99的估值。



#### 9.4 线性（回归）法

​	在估值函数中最重要的函数之一就是估值函数$\hat v(.,\theta)$是基于权重向量$\theta$的线性函数。对任意$s$，皆存在实特征向量$\phi(s)\doteq(,\phi_1(s),\phi_2(s),...,\phi_n(s)^T$，具有与$\theta$相同的分量数。特征向量可通过不同方法从状态中构造出来。在下一节我们给出了一些选择。而无论特征如何构造，状态估值函数都是有$\theta$和$\phi(s)$的内积给出的：

$$
\hat v(s,\theta)\doteq\theta^T\phi(s)\doteq  \sum_{i=1}^n\theta_i\phi_i(s) \tag{9.8}
$$
在该式中称估值函数和权重具有线性相关性，简单地说就是线性相关的。其中的独立函数$\phi_i:\mathcal S \to \mathbb R$被称为“基函数”，因为它们构成了基于该（线性）形式的一系列估值函数的基础。	构造$n$维特征向量来表示状态等同于选择一组有$n$个基函数的函数组。

​	自然地，我们会想到在线性函数拟合中使用$\rm SGD$更新。该例中基于$\theta$的估值函数的梯度是：

$$
\nabla\hat v(s,\theta)=\phi(s).
$$
因此，在线性的情况下，一般$\rm SGD$更新（9.7）就简化为上式这种特殊形式了。

​	由于其足够简单，线性$\rm SGD$成了人们最喜欢用于数学分析的方法。几乎所有用于各类学习系统的实用收敛结果均适用于线性（或更简单的）函数拟合法。

​	特别地，在线性方法中只有一个最优解（或者在不理想的情况下，有一组同样好的最优解），所以这种情况下任何一种能保证收敛至近似或者局部最优的方法皆等同于收敛至全局最优。例如，基于一般条件，即学习率$\alpha$随时间递减的话，那么前面章节所述蒙特卡罗算法的梯度就能通过线性函数拟合来收敛至$<span class="">\</span>rm MSVE$的全局最优解。

​	前节所述的半梯度$\rm TD(0)$法也能通过线性函数拟合收敛，但其收敛结果和一般$\rm SGD$的结果不同；关于这点需要用另一个定理来说明。此外，该方法下最终收敛的权重向量也不是全局最优，而是一个靠近局部最优的解。该方法很重要，尤其是在持续性任务中，因而有很多细节需要深入思考。例如基于该方法，在每个时刻$t$的的更新可表示为：

$$
\begin{eqnarray}
\theta_{t+1}&\doteq&\theta_t+\alpha(R_{t+1}+\gamma\theta^T_t\phi_{t+1}-\theta^T_t\phi_t)\phi_t \tag{9.9}\\
&=&\theta_t+\alpha(R_{t+1}\phi_t-\phi_t(\phi_t-\gamma\phi_{t+1})^T\theta_t),
\end{eqnarray}
$$
其中我们使用符号简写表示$\phi_t=\phi(S_t)$。当该系统达到稳定的状态后，对于任意给定$\theta_t$，下一次更新的期望权重向量可写为：

$$
\mathbb E[\theta_{t+1}|\theta_t]=\theta_t+\alpha(b-A\theta_t),  \tag{9.10}
$$
其中 

$$
b\doteq\mathbb E[R_{t+1}\phi_t] \in \mathbb R^n ， A\doteq\mathbb E[\phi_t(\phi_t-\gamma\phi_{t+1})^T] \in \mathbb R^n \times \mathbb R^n \tag{9.11}
$$
从（9.10）可见，若该系统是收敛的，其一定收敛于权重向量$\theta_{TD}$，其中：

$$
\begin{eqnarray}
b-A\theta_{TD}&=&0\\
\Rightarrow \qquad\qquad\qquad  b&=&A\theta_{TD}\\
\Rightarrow\qquad\qquad\,\,\,\,\, \theta_{TD}&\doteq&A^{-1}b.\tag{9.12}
\end{eqnarray}
$$
该量（quantity）被称作TD固定点（ TD fixpoint）。事实上线性半梯度TD(0)法收敛于该点。一些理论证明了其收敛性。而下图框中给出了上式中逆矩阵的存在证明。(dbsx)

> ---
>
> 线性TD（0）方法的收敛性证明
>
> ---
>
> 什么样的属性才能保证 线性TD（0）方法（9.9）的证明呢？将式（9.10）重写为如下形式可以获得些灵感：
> $$
> \Bbb E[\theta_{t+1}|\theta_t]=(\rm I-\alpha A)\theta_t+\alpha b.\tag{9.13}
> $$
> 注意，矩阵$A$是和权重向量而不是$b$相乘的；只有$A$对收敛有影响。为了便于理解，考虑$A$为对角矩阵这种特殊情况。若所有的兑奖元素为负，则其相应的的矩阵元素$\rm I-\alpha A$会比一大，随之相应的$\theta_t$的元素会被放大。如果更新持续进行，最终导致结果发散。另一方面，若$A$的所有对角元素均为正，则$\alpha$可选择一个比 $1/X$(译者：$X$为$A$对角元素中的最大值)更小的值，如此同其他介于0到1间的所有对角元素，$\rm I-\alpha A$也成为了对角元素。 在该情况下式中第一项会趋向于压缩$\theta_t$，如此便保证了结果的稳定性。一般情况下，当A是正定的，即对所有实向量$y$有$y^{\sf T} Ay>0$时，$\theta_t$会被递减至0。正定性同样保证了逆矩阵$A^{-1}$的存在。
>
> ​	对于线性TD（0），在$\gamma<1$的持续性任务中，矩阵$A$(9.11)可写为：
> $$
> \begin{eqnarray}
> A&=&\sum_sd(s)\sum_a\pi(a|s)\sum_{r,s'}p(r,s'|s,a)\phi(s)(\phi(s)-\gamma\phi(s'))^\sf T\\&=&\sum_s d(s)\sum_s'p(s'|s)\phi(s)(\phi(s)-\gamma\phi(s'))^\sf T\\&=&\sum_sd(s)\phi(s)\left(\phi(s)-\gamma\sum_s'p(s'|s)\phi(s')\right) ^\sf T\\&=&\Phi^\sf T\rm D(\rm I-\gamma P)\Phi,
> \end{eqnarray}
> $$
> 基于策略$\pi$，$d(s)$代表平稳分布，$p(s'|s)$是从$s$到$s'$的转移概率，$\rm P$是这些概率的$|\sf S|$行$|\sf S|$列矩阵，$\rm D$是$|\sf S|$行$|\sf S|$列的对角矩阵，其中$d(s)$是其对角项。而$\Phi$是$|\rm S|$行$n$列的矩阵，其中$\phi(s)$是其行（向量）。从这里能很清楚地看到内矩阵$\rm D(\rm I-\gamma P)$对$\rm A$的正定性起关键作用。
>
> ​	诸如此类的关键矩阵，只要其所有列之和为非负数即可保证正定性。这是由Sutton (1988, p. 27)基于上述2条定理而提出的。定理1：对任意矩阵		$\rm M$,当且仅当其对称矩阵$\rm S=\rm M+\rm M^\sf T$是正定的，则$\rm M$为正定矩阵。定理2：对任意对称实矩阵$\rm S$,若其对角元素值均为正，且（任意一个对角元素）值比其相应的非对角元素之和大(Varga 1962, p. 23)，则$\rm S$为正定矩阵。这里我们的关键矩阵$\rm D(\rm I-\gamma P)$，其对角元素为正，其他元素为负。所以只要证明每行之和加上对应的列之和为正即可。因为$\rm P$是随机矩阵且$\gamma <1$所以行之和一定为正。那么现在仅需证明列之和为正。注意，对于任意矩阵$\rm M$,由其列之和构成的行向量均写成$1^\sf T\rm M$,其中$\rm 1$是元素均为1的列向量。记$\rm d$为基于分布$d(s)$的$|\sf S|$向量。因为$\rm d$是平稳分布，所以$d=\rm P^\sf T\rm d$。 那么现在我们的关键矩阵的 列之和可表述为：
> $$
> \begin{eqnarray}
> \rm 1^\sf T\rm D(\rm I-\gamma\rm P)&=&\rm d^\sf T(\rm I -\gamma \rm P)\\&=&\rm d^\sf T-\gamma \rm d^\sf T\rm P\\&=&\rm d^\sf T-\gamma\rm d^\sf T  \qquad（因为d是平稳分布）\\&=&(1-\gamma)\rm d,
> \end{eqnarray}
> $$
> 该关键矩阵的所有元素均为正。因此，该矩阵及其$A$矩阵均为正定矩阵，而在策略TD(0)法也被证明是稳定的。（若要证明其收敛概率为1，需要一些额外条件以及对随时间递减的$\alpha$的调整。）

---

​	当$θ$在$\rm TD$固定点，其$\rm MSVE$已被证明（在持续的情况下）仅在最小可能误差的有限范围内波动：

$$
\rm MSVE(\theta_{TD})\leq{1\over1-\gamma}\min_\theta\rm MSVE(\theta).\tag{9.14}
$$
也就是说，$\rm TD$法的近似误差（ asymptotic error）小于等于$1\over1-\gamma$乘以最小可能误差的值。这个最小可能误差在蒙特卡罗法的极限中可获得。由于$\gamma$往往近似于1，该膨胀因子会很大。因此$\rm TD$法的渐近性能会有一定的潜在损失。另外，正如我们在第6,7章看到的那样---由于TD法往往比蒙特卡罗法更广泛地降低了估值方差，所以其学习得更快。具体该用哪种方法取决于问题和相应函数拟合方法的属性，以及学习所需的时间。

​	（9.14）的约束也适用于一些其他的在策bootstrapping法。例如，根据在策略分布来备份的线性半梯度$\rm DP$法（例如：式9.7，其中$U_t\doteq\sum_a\pi(a|S_t)\sum_{s',r}p(s',r|S_t,a)[r+\gamma\hat v(s',\theta_t)]$）也会收敛至$\rm TD$固定点。而在单步半梯度“动作值“（action-value）类方法中，诸如下章提到的半梯度$\rm Sarsa(0)$法，它们也有个类似的$\rm TD$固定点和约束。回合制任务情况会略有不同，但也有类似约束（详见 Bertsekas and Tsitsiklis, 1996）。为了满足约束，需要对回馈、特征及步长参数的递减做出一些技术约束，在此我们不做讨论。欲知详情请参考原论文（Tsitsiklis and Van Roy, 1997）。

​	保证这些收敛结果的关键在于备份的状态是服从在策分布的。对于其他状态分布，使用函数拟合的bootstrapping法可能会导致结果发散至无穷。我们将在第11章论讨论关于该情况的一个例子和对应解决方案。

**示例 9.2：在1000-状态随机步行中使用bootstrapping**  状态聚合是使用线性函数拟合法的一种特殊情况。那么结合本章的内容，我们来看下在1000-状态随机步行任务中能有哪些发现。 图9.2的左图展示了经由状态聚合（例9.1）的半梯度$\rm TD(0)$法（p197）学习而来的最终状态值函数。可见$\rm TD$法的估值误差远大于蒙特卡罗法（图9.1）。

​	然而，$\rm TD$法在学习效率上有着很大的潜在优势。正如第7章所述，其也将纯蒙特卡罗法推广至多步$\rm TD$法。在该步行任务中，从图9.2的右图可见，使用了状态聚合的半梯度$\rm TD$法和之前在19-状态随机步行任务中的表格法在结果上有着惊人的相似。这里为了保证数量上的相似性，我们将状态聚合改为20分组，每组50个状态。因为20个分组在数量上近似于表格法中的19个状态。

![](img\Figure 9.2.png)

$图 9.2：在1000-状态随机步行任务中使用状态聚合的bootstrapping法。\\相较于图 9.1中的\rm MC估值半梯度\rm TD的渐近估值要糟糕一些。图右侧：结合状态聚合，\\使用了n步\rm TD法和图7.2的表格法中的结果有着惊人的相似。$



> ---
>
> 用$n$步半梯度$\rm TD$法估计$\hat v\approx v_\pi$
>
> ---
>
> 输入：用于评估的策略$\pi$
>
> 输入：可微函数$\hat v:\mathcal S^{+}\times \mathbb R^{n}$，其中$\hat v(最终状态, . )=0$
>
> 参数：学习率 $\alpha\in(0,1]$，正整数$n$
>
> 任意初始化估值函数的权重$\theta$ (比如：$\theta=0$)
>
> （每个回合）重复：
>
> ​	初始化并储存$S_0\neq \rm terminal$
>
> ​	$T \leftarrow \infty$
>
> ​	当$t=0,1,2,...:$
>
> ​	|	若$t<T$，则：
>
> ​	|		根据$\pi(.|S_t)$选择一个动作
>
> ​	|		观察并储存下一个回馈$R_{t+1}$和下一个状态$S_{t+1}$
>
> ​	|		若$S_{t+1}$是终点，则$T\leftarrow t+1$
>
> ​	|	$\tau \leftarrow t-n+1\quad$ （$\tau$所在的状态估值将被更新）
>
> ​	|	若$\tau\geq0:$
>
> ​	|		$G\leftarrow \sum^{min(\tau+n,T)}_{i=\tau+1}\gamma^{i-\tau-1}R$
>
> ​	|		若$\tau+n< T$，则$G\leftarrow G+\gamma^n\hat v(S_{\tau+n},\theta)\qquad	 (G^{(n)}_\tau)$
>
> ​	|		$\theta\leftarrow\theta+\alpha[G-\hat v(S_\tau,\theta)]\nabla\hat v(S_\tau,\theta)$
>
> ​	直到$\tau=T-1$	
>
> ---

特别地，使用状态聚合的状态分组的组状态转移概率（指转移到相邻的分组状态，平均每50，或最多100个状态为一组）也和使用表格法的单状态转移概率类似。为了类比，这里我们使用和表格法任务中同样的$\rm RMS$误差指标来衡量前10个回合中所有状态的表现。不用$\rm MSVE$是因为其更适合于函数拟合的情况。	

​	将第7章所述的$n$步$\rm TD$算法融入半梯度函数拟合，就自然成为了我们在示例中所用的半梯度$n$步$\rm TD$法了。类似于（7.2），其关键方程是：

$$
\theta_{t+n}\doteq\theta_{t+n-1}+\alpha\left[G_t^{(n)}-\hat v(s_t,\theta_{t+n-1}) \right]\nabla\hat v(S_t,\theta_{t+n-1}),\,0\leq t<T,\tag{9.15}
$$
其中的$n$步回报是从（7.1）式推广而来的：

$$
G_t^{(n)}\doteq R_{t+1}+\gamma R_{t+2}+...+\gamma^{n-1}R_{t+n}+\gamma^n\hat v(S_{t+n,\theta_{t+n-1}}),0\leq  t<T,\tag{9.16}\\
$$
上面的框中给出了该算法完整伪代码。



### 9.5 线性法中的特征构造

​	线性法的有趣之处不仅在于其可收敛性，也在于实践中其能高效的处理数据和计算。是否真的能如此高效的关键在于任务状态是如何被特征表示的，而这也是本大节我们需深入研究的。选择合适的特征是给强化学习系统增加先验知识的一个重要途径。直观上，选择的特征应该和任务的自然属性有一定联系，带有这些属性的特征也意味着其有最好的泛化能力。举例来说，若我们在评估一个几何体，我们可以选择的特征有几何体的形状，颜色，尺寸和功能。如果我们评估的是移动机器人的状态，则我们也许需要的特征包括（机器人）位置，剩余电量，最近的声纳数据等等。

​	一般来说，我们也需要这些自然属性的组合特征。这是因为线性模式屏蔽了各个特征间的互动作用。比如某个特征$i$只有当另个特征$j$不存在时才有效。举例来说，在平衡木任务（示例3.4）中，一个高角速度是好是坏取决于当前的角度。如果角度高，则高角速度意味着危险，因木棒即将坠落---所以是个坏状态。然而若角度低，则高角速度意味着木棒正在纠正自己的位置---所以是个好状态。若使用线性函数拟合，则在这种有着特征互动的情况下，需要引入组合特征来估算状态值。在接下来的小节里我们将讨论各种组合特征的方法。



#### 9.5.1 多项式

对于多维连续状态空间，用于强化学习的函数拟合和内插回归问题有着诸多相似之处---均是基于给定样本来定义函数。很多普遍用于求解内插回归的多项式也可以被用于强化学习任务。这里我们仅讨论最基本的一些多项式（形式）。

​	假设某个强化学习问题中的状态空间是由$s=(s_1,s_2)^{\sf T}$的组成的二维实向量。为了能引入特征间的交互作用，你可能会通过将权重相乘$s_1s_2$，以一种合适的方式，如$(1,s_1,s_2,s_1s_2)^{\sf T}$来表示每个状态。或者可以引入更复杂的特征组合，选择诸如$(1,s_1,s_2,s_1s_2,s^2_1,s^2_2,s_1s^2_2,s^2_1s_2,s^2_1s^2_2)^{\sf T}$的特征向量。使用这些组合特征使得函数以多元二次的形式进行拟合----尽管由那些需要学习的权重组成的拟合函数依然是线性的。

​	这些示范特征向量是在选择了一组多项式基函数后的特定结果。而这些基函数可以被定义为任意维度，并可在状态变量中引入高维复杂特征组合：

> ---
>
> ​	对于用$d$个实数变量来表示一个状态的情况，每个状态$s$就是一个$d$维实数向量$(s_1,s_2,...,s_d)^{\sf T}$。每个$d$维多项式基函数$\phi_i$可写为：
>
> $\phi_i(s)=\prod^d_{j=1}s^{c_i,j}_j,\tag{9.17}$
>
> 其中每个$c_{i,j}$是集合$\{0,1,...,N\}$，$N≥0$的一个整数。这些函数组成了$N$阶多项式基，包含了$(N+1)^d$个不同的函数。
>
> >---

​	高阶多项式允许对更复杂的函数进行更精准的拟合。但由于$N$阶多项式基的函数数量随着状态空间$(N＞0)$成指数级增长，所以一般有必要筛选一个子状态集用以函数拟合。通过观察被拟合的函数属性，借助先验知识（belief），选择一个子状态集是可行的。而另一些在多项式回归中发展而来的自动选择方法则可适用于处理具有递增、非静态属性的强化学习任务。

**测验9.1**  为何式（9.17）要为$d$维状态空间定义$(N+1)^d$个不同的函数？

**测验9.2** 给出用以产生特征向量$(1,s_1,s_2,s_1s_2,s^2_1,s^2_2,s_1s^2_2,s^2_1s_2,s^2_1s^2_2)^{\sf T}$的基函数定义元素---$N$和$c_{i,j}$。



#### 9.5.2 傅里叶基

​	另一种线性函数拟合是基于时间的傅里叶级数（Fourier series）。其告诉我们任意一个周期函数均可表示成以三角函数$sine$或者$cosine$为基函数的不同级别的加权和。（若$f(x)=f(x+T)$，则对所有$x$，$f$是以$T$为周期的函数）傅里叶级数和更一般的傅里叶变换被广泛地用于应用科学。因为若被拟合函数是已知的，则往往其基函数的权重项可以（are 一定？）用简单的形式表示。而且只要有足够多的基函数，任何函数估计均可被精确到预期。有意思的是，尽管在强化学习任务中被拟合的函数是未知的，但傅里叶基函数依然很好用，且在很多任务中表现不错。Konidaris, Osentoski, 和Thomas 在2011年展示了一个由简单的傅里叶基构造的函数可以被用于多个由多维持续性状态空间构成的强化学习问题，且该函数并不需要具有周期性。

​	首先考虑下单维度的情况。由一般福利叶级数构成的一维函数有一个周期$T$，通过该周期将估值函数表示为正余弦函数的线性组合，而其中每个正余弦函数的周期可被$T$整除（换言之，其周期可由一个整数乘以某个基准周期$1/T$来表示）。不过若被拟合的是个定义在有界区间的非周期函数，则可以将傅里叶基函数的周期$T$设为整个区间的长度。可见这类由正余弦基函数构造的线性组合的特别之处在于其仅含有一个周期。

​	而且，若将周期设为目标区间的2倍且仅关心半个周期区间$[0,T/2]$中的估值。

![](img\Figure 9.3.png)

$图9.3：通过一维傅里叶基函数\phi_i=1,2,3,4来拟合在区间[0,1]间的函数；\phi_0是个常量函数。资料来自Konidaris等人（2011），已获得授权。$



那么用余弦基函数即可。这是可行的，因为任意“偶”函数均可由其表示。即仅用余弦基函数可表示任意轴对称函数。所以只要有足够多的余弦基函数，在半周期$[0,T/2]$区间的任意函数均可被近似估计到期望精度。（这里的“任意函数”并不完全正确，因为严格来说必须保证函数是 mathematically well-behaved的。但这里我们忽略不计。）另外，也可仅用正弦基函数，一个全部由“奇”函数（非原点对称）组成的线性组合（来模拟估值函数）。不过一般还是用余弦基函数更好，因为“半偶”函数比“半奇”函数更容易模拟，鉴于后者往往在原点（origin）不连续。

​	遵循该逻辑将$T=2$,于是得函数的定义域为$T$的一半$[0,1]$,那么由$N+1$个函数组成的单维N阶傅里叶余弦基函数就是：
$$
\phi_i(s)=cos(i\pi s), s \in[0,1]
$$
其中$i=0,...,N$。图9.3展示了单维N傅里叶余弦基函数$\phi_i$，其中$i=1,2,3,4$；$\phi_0$是个常数函数。与多项式基函数不同的是傅里叶基函数总是有界的，不需要求幂（do not require exponentiation）。

​	在多维的情况下，傅里叶余弦级数也有该性质。

> ---
>
> 对于一个由$d$阶超矩阵（原点元素在某个角）构成的状态空间，状态均为实向量$s=(s_1,...,s_d)^{\sf T},s_i \in [0,1]$。$N$阶傅里叶余弦基函数中每个基函数可写为：
> $$
> \phi_i(s)=cos(\pi c^i.s), \tag{9.18}
> $$
> 其中$c^i=(c^i_1,...,c^i_d)^{\sf T}$，$c^i_j\in \{0,...,N\}$，$j=1,...,d$且$i=0,...,(N+1)^d$。该公式为每个有$(N+1)^d$个可能的实向量$c^i$定义了一个函数。其点积$c^i.s$就是将$\{0,...N\}$内的整数分别乘到每个维度中去。正如在单维情况下那样，该整数决定了（估值）函数在该维度的频率。当然，为了适用于某个具体应用的状态空间，可以通过位移或缩放来调整基函数。

------

![](img\Figure 9.4.png)

$图9.4  \quad2维傅里叶余弦基函数\phi_i，i=0,1,2,3,4,5。来自Konidaris 等人(2011)，已获得授权。$



​	作为一个示例，考虑$d=2$的情况。其中$s=(s_i,s_2)$，其中每个$c^i=(c^i_1,c^i_2)^\sf T$。图9.4展示了基于6个傅里叶余弦基函数的情况。其中每张图由向量$c^i$定义（$s_1$代表纵轴而$c^i$代表行向量，$i$被省去了）。$c$中的任意个维度为0代表函数在该维度上的值恒定。所以若$c=(0,0)$，则函数在2个维度的值都是恒定的；若$c=(c_1,0)$，则函数在第二维度的值是恒定的，而其值随着第一维度的频率而波动；$c=(0,c_2)$的情况也类似。若$c=(c_1,c_2)$，其中没有一个维度（$c_j$）为$0$。则基函数值随着2个维度同时波动，其（值变化）反映了2个状态变量间的互动。$c_1$和$c_2$给出了每个维度的频率，而它们的比值给出了互动的方向( direction of the interaction)。 Konidaris等人在2011年发现了一个现象---若在诸如学习算法（9.7）半梯度$TD(0)$，或者半梯度Sarsa$(\lambda)$中融入傅里叶余弦基函数，则为每个基设置一个不同的步长参数会更好。若基函数的步长参数是$\alpha$，则他们建议将其每个基函数$\phi_i$的步长设置为$\alpha_i=\alpha \sqrt{(c^i_1)^2+...+(c^i_d)^2} $(除非每个$c^i_j=0$，若如此则$\alpha_i=\alpha$)。经发现，若在强化学习任务中使用$Sarsa(\lambda)$搭配傅里叶余弦基函数，则其表现要优于搭配其它基函数，比如多项式基和径向基。但不出所料地，在处理中断（discontinuities）的情况上傅里叶余弦基函数有些问题。这是因为除非有非常高频的基函数，否则其很难避免在中断点附近的“波动”（ringing）。

​	正如多项式基函数，在$N$阶傅里叶余弦基中的基函数数量也将随着状态空间的维度增加而呈指数级膨胀。因此，若状态空间维度过高（如：$d>5$），则必须选择基函数集的一个子集来拟合。

![](img\Figure 9.5.png)

图9.5 	在1000状态随机步行任务中，傅里叶基与多项式基的对比。该图展示了基于傅里叶基和多项式基，degree分别为5,10,20时$\rm MC$法的学习曲线。经过粗略地优化，2者的步长参数$\alpha$分别设置为0.00005和0.0001。



根据被估函数的属性来先验地选择是可行的。而一些自动选取基函数的方法则可适应处理迭代的、非静态的的强化学习任务。从这个角度来说，傅里叶余弦基函数的一个优势在于---其通过设置向量$c^i$来解释状态变量间的一些可疑交互作用便可方便地选择基函数；而通过限制向量$c^j$的取值范围，可在估值中将一些可能是噪音的高频元素剔除出去。

​	图9.5展示了分别使用傅里叶基和多项式基在1000状态随机步行任务的学习曲线。一般来说，若使用在线学习法，不推荐使用多项式基。

**练习 9.3**  为什么式（9.18）为$d$维空间定义了$(N+1)^d$个不同的函数？



#### 9.5.3 粗编码/稀疏编码（Coarse Coding）

​	考虑这样一个任务---其状态是连续且二维的。那么该任务的一个状态就是个2维空间的点（ a point in 2-space），即2个实数组成的向量。我们可以用“圆”（circles）来表示状态空间中的相应特征，如图9.6所示。如果指定的状态在一个圆中，则该圆所代表的特征值为1，并称该特征是“存在的”（present）；否则特征值为0，并称该特征是“不存在的”（absent）。这种1-0值的特征被称为二值化特征。给定一个状态，通过观察存在的二值化特征，可以确定该状态处于哪些圆中。据此可对其位置给予粗略的编码。我们称这种以重叠特征的方式来表示状态的方法（尽管特征并不需要是圆或二值化）为“粗编码/稀疏编码”。

​	假设使用基于线性梯度下降的函数拟合法，考虑圆的尺寸和密度对其的影响会发现

![](img\Figure 9.6.png)

**图9.6 粗编码。那些感受野（本例中为圆）重叠的特征圆的数量会影响到状态$s$到$s'$的泛化。该示例中两个状态共有一个共同特征，因而两个状态的泛化会稍稍受到彼此间的影响。**



---每个圆的相应权重（$\theta$），会在学习过程中发生改变。可见当训练一个状态时（空间中的一个点），和该状态交织在一起的圆权重皆会发生改变。因此根据（9.8），所有处于这些（译者：被影响到权重的）圆中的状态估值均会发生改变。一个状态点所具有的的“普遍”特征（圆）越多，其对其他状态估值的影响就会越大，如图9.6所示。如果圆比较小，则其泛化影响会在一个较小的区域内，正如图9.7a所示。反之若较大，则其泛化的影响会在一个较大的区域内，正如图9.7b所示。且特征的形状也会决定泛化的本质。如图9.7c所示，若特征的形状不是严格意义上的圆，而是在某个方向上有所拉伸的圆，则其泛化后（特征圆的形状）也会有类似特征。

​	有较大感受野的特征会被给予更广泛的泛化，但这会造成学习到的估值函数过于粗略---也就是无法给出比感受野的宽度更精细的辨识度（discrimination）。幸可喜的是，这并不是什么大问题。因为从一个点到另一个点的泛化，虽然一开始确实是受到感受野的大小及形状的影响。

![](img\Figure 9.7.png)

**图9.7：感受野的大小和形状会影响到线性函数拟合中的泛化。大致来说，上图的3种情况有同样数量的特征和特征密度。**



但锐度（acuity），即状态间最细微的差别最后完全可能被状态的总特征数所主导。

**示例 9.3： 粗编码的粗粒度**	本示例将阐述感受野尺寸是如何在粗编码中影响学习效果的。该示例将用基于粗编码和（9.7）的线性函数拟合法来学习一个一维方形波浪函数（图9.8顶部所示），其中将该函数值作为目标值$U_t$。由于只有一维，感受野是个（一维）区间而不是（二维）圆。如图底部所示，我们分别通过3个尺寸的区间（窄，中，宽）来观察各自的学习效果。3个示例有着相同数量（density）的特征，比需要学习的函数数量（extent）多50个左右。训练样本均匀地从该数量的样本集中随机抽取。步长参数为$\alpha={0.2\over m}$，其中$m$代表某一刻存在的特征数量。图9.8展示了在所有三个示例中函数的学习过程。注意，在学习初期特征的宽度对学习确实有很大的影响。较宽的特征，意味着泛化效果也会较宽；较窄的特征，则意味着仅距离训练点较近的几个点的估值会被影响，这也使得学习到的函数更加的颠簸。然而，特征的宽度对最终习得的函数仅有轻微的影响。（可见）感受野的形状对泛化有很强的影响，但对渐近精度的质量影响却很小。

![](img\Figure 9.8.png)

**图9.8 特征的宽度对初期泛化（第一行）有较强影响，但对渐近精度的影响却不大（最后一行）。**



#### 9.5.4 瓦片编码（Tile Coding）

​	瓦片编码是用于多维连续空间的一种粗编码形式。其具有计算上的高效性及灵活性。对于现在的顺序数字计算机（ sequential digital computers），其可能是最实用的特征表示方法。开源软件中有很多使用瓦片编码的案例。

​	在瓦片编码中，特征的感受野被组织为输入空间的分区（元素集）。我们称每一个分区为一个“瓦块”（tiling），并称分区的每个元素为“瓦片”（tile）。例如图9.9左侧所示，作用于二维状态空间的最简单的瓦块可以是个均匀的网格。这里的瓦片或者说感受野是方块状的而不是像图9.6中的圆。若只使用1个瓦块，则只需用图中白点所处的瓦片来表示其状态即可；处于同一瓦片的所有状态均收到泛化影响，而其他状态则不会。仅仅使用1个瓦块，其实际等同于状态聚合而不是本章所说的粗编码。

​	为了利用粗编码的优势，我们需要重叠的感受野。而根据定义，一个瓦块中的瓦片是不会重叠的。那么为了在瓦片编码中做到真正的粗编码，我们需要用到多个不同的瓦块。不同瓦块彼此间偏移一定距离，该距离为一瓦片的部分宽度。在图9.9的右侧是个使用了4个瓦块的简单示例。每一状态，如图中的白点，对应于每个瓦块中的一个瓦片。当该状态出现时，相对应的4个瓦片会被激活为该状态的4个有效特征。具体来说，特征向量$\phi(s)$为每个瓦块中的每个瓦片配置了一个元素。那么在本例中就共有$4\times4\times4=64$个元素，除了$4$个置于$s$的瓦片外，其余所有瓦片值均为$0$。图9.10展示了在1000步随机步行任务中多重偏移瓦块（粗编码）比单瓦块的优势。

​	由于瓦片是基于分区工作的。所以在实际任务中关于瓦片编码的直接优势在于用于表示任一时刻的任一状态的激活特征数量都是相同的。正因为每一瓦块中只有一个瓦片是被激活的，所以用以表示某一状态的总特征数就是瓦块数。这样的特性使我们可以很容易地依直觉来设置步长参数$\alpha$。例如，选择$\alpha={1\over m}$，其中$m$是瓦块的总数。这样使其转化为了 one-trial learning。在$s\to v$的样本训练中，无论之前的估值$\hat v(s,\theta_t)$是什么，新的估值将是$\hat v(s,\theta_{t+1})$。一般为了更好地泛化和避免随机波动，可以将$\alpha $调得更慢一点。例如，我们可选择$\alpha={1\over 10m}$。那么在该例的每一次更新迭代中，每一次训练样本的估值会以十分之一的幅度靠近目标值。

![](img\Figure 9.10.png)

图9.10

**图	9.10：为何使用粗编码。上图展示了在1000状态随机步行任务中，梯度MC算法中单瓦块和多重瓦块的效果差别。我们视该任务中的1000个状态为一个一维连续状态空间。每一个瓦片覆盖200个状态。在多重瓦块中，瓦块彼此间的偏移距离为4个状态。这里也设置了步长参数，这样能保证两者在初始阶段的学习率一致。其中单瓦块中设$\alpha=0.0001$，多重瓦块中设$\alpha=0.0001/50$。**

而其周边状态的更新幅度会较小，该幅度大小与状态彼此间共享的瓦片数成比例。

​	由于使用二值化特征向量，瓦片编码也具有了计算优势。因为每个元素非0即1，所以几乎不费吹灰之力即可通过权重求和算出式（9.8）中的函数估值了。相较于做$n$次乘法和加法，使用瓦片编码的好处在于其仅需计算$m\ll n$个激活的特征，并将相应的$m$个权重向量元素进行求和即可。

​	除了被训练的那个状态，瓦片编码的泛化也作用于其他落在同一组瓦片中的状态（集）。收到影响的状态总数一般和瓦片数成正比。甚至瓦块间彼此的偏移方式也会影响到最终的泛化效果。如图9.9所示，如果瓦块彼此在不同维度均匀地偏移相同的距离，则不同的状态会有不同的泛化质量。如图9.11上半部分所示，这8张子图皆展示了一个被训练的状态是如何因泛化模式的不同而影响到其周边状态的。在该例中总共有8个瓦块，因此64个子域（译者：为何是64个？）中会有一个瓦片被直接泛化。但其泛化的模式一定是图中8个中的一种。注意到依赖于平均偏移，其所构成的泛化对对角上的状态集有着很强的影响。为了避免该人为特征，只要保证瓦块在每个维度上非对称地偏移即可。这种泛化模式（比平均偏移）更好，因为所有瓦片都围绕着被训练的那个状态，并没有明显的不对称。

​	无论哪种情况，所有瓦块的偏移距离都是单个瓦片在该维的宽度的一部分。令$w$表示瓦片的宽度，$k$表示瓦块的数量，则$w\over k$就是偏移距离的基本单元。在某个（方向、维度）小方格$w \over k$内的所有状态会被相同组的瓦片被激活，即具有相同的表征（ feature representation），以及相同的估值。若某状态在任意笛卡尔方向上偏移了$w\over k$个位移，则对其表征的瓦片中的一个会被改变。而平均偏移的瓦

![](img\Figure 9.11.png)

**图 9.11：为何采用非对称地方式偏移？在上图的8个瓦块中针对某个训练状态（图中的黑色加号）及其周边状态集，可见若以均匀方式偏移（图中上半部分），则泛化结果上有很强的对角效应，并伴随不可忽视的波动；反之，若使用非对称方式，则结果会显得更加同质而圆满（spherical）	**



块彼此间正好偏移了一个单元距离。以二维状态空间为例，当我们说每个瓦块偏移了$(1,1)$个位移矢量（ displacement vector），即意味着每个瓦块和其前瓦片的偏移距离等于$w\over k$乘以该矢量。按照这种说法，图9.11下半部分的非对称瓦块的偏移矢量就是$(1,3)$。

​	关于不同的位移矢量对瓦片编码的泛化影响，已有诸多学者对此作了广泛的研究。如之前看到的偏移了$(1,1)$个位置的矢量对角特征， (Parks and Militzer, 1991; An, 1991; An, Miller and Parks, 1991; Miller, Glanz and Carter, 1991)对其同质性和趋势作了相应评估。根据他们的研究，  Miller and Glanz (1996)推荐使用由第一奇整数（ the first odd integers）组成的偏移矢量。特别地，对任意$d$维连续状态空间，位移矢量应取第一奇整数$ (1,3,5,7,...,2d − 1)$，而$k$（瓦块的数量）的取值应该保证$2$的$k$次方大于等于$4d$。其效果正如图9.11下半部分所示。该图中$d = 2, k = 2^3 ≥ 4d$，位移矢量为$(1,3)$。而在三维状态空间中，前四个瓦块的偏移应该是$ (0,0,0), (1,3,5),(2,6,10)$和$(3,9,15)$。一些开源软件可以根据预先设置的$d$和上述规则来高效设置瓦块。

​	若选择瓦片编码，则需设置瓦块数量和瓦片的形状。瓦块的数量和瓦片大小决定了拟合的近似精度，如图9.8中一般的粗编码所展示的那样。而瓦片的形状则决定了泛化的特性（nature），如图9.7。可见，方形瓦片的泛化效果在每个维度上是大致相等的，如图9.11（下半部分）。若瓦片沿某方向拉伸，如图9.12b中的条状瓦片，则该维度上的状态会被更多地泛化。可见，图9.12b中的条形边界在左边显得更密集也更薄，从而该维（水平）方向上较低值的瓦片会有更好的区分度。另外，图9.12c的对角条瓦片则是基于某个对角，促使泛化沿着该对角线变化。在更高的维度中，平行于某坐标的条状瓦片会忽视掉部分瓦块中的部分维度，即超平面切片。图9.12a中的不规则瓦块虽然也是一种选择，但很少在实际中使用，标准软件也无法驾驭。

​	实际应用中，搭配不同的瓦块，配以不同形状的瓦片做法是更好的选择。例如，同时使用一些水平条状瓦块和垂直条状瓦块，这样可鼓励在每个维度进行（相同程度的）泛化。然而，若在某个特定水平和垂直条内的坐标有个特别的值，若仅用条状瓦块，这种情况的估值是无法学习到的（因为无论学到了什么都会渗透到相应的水平或垂直条内的所有状态集中）。为了做到这点，需要一些交织型的瓦块，如9.9所示的连接矩形瓦片就是一种选择。而当有了垂直的、水平的以及一些交织形态的瓦块后，所有问题都解决了---对任意维度都有相应的泛化偏好，而又不失对某些连接点内特殊值的学习能力（16.3节有一个使用该方法的案例）。对瓦块的选择会影响到泛化的效果，因此在还无法有效自动化该选择前，有必要保证我们选择的瓦片编码具有足够的灵活性和可读性。

​	![](img\Figure 9.12.png)

​	**图 9.12：瓦块并不需要做成网格状的。在不失计算效率的前提下，其也可由任意形状，任意大小的瓦片组成。**

另一个降低内存需求的技巧是”哈希“（hashing）---通过一个伪随机法将较大的瓦块分解为一个有诸多小很多的瓦片组成的集合。![Hash](img\Hash.png)哈希生成的瓦片是由遍布在状态空间中诸多非连续、解耦的区域组成的，但这样依然过于庞大（译者：这句话和下面的转折组合有点奇怪）。例如，一个瓦片可以由四个子瓦片组成，如右侧图。通过哈希往往仅需消耗少量的性能即可极大地降低内存需求。之所以可行是因为仅状态空间中的一小部分才需要足够高的精度。可见通过哈希可解决维数灾难，因为任务所需的内存不再是状态空间维数的指数级大小了，而仅需某满足实际任务需求。现在已经有很多足够好的开源软件可以操作带有哈希的瓦片编码了。

**练习9.4**	在某二维状态空间中，若已知某个维度比另一个维度对估值函数有更大的影响，则为了更好的泛化效果，瓦块应该贯穿而非平行于该维度。那么请问哪种瓦块才能利用好该先验知识呢？



#### 9.5.5 径向基函数

​	径向基函数（$\rm RBFs$）是粗编码中应用于连续值特征的一种自然泛化。该方法中的特征取值不再仅限于$0$或者$1$，可取在区间$[0,1]$间的任意值。这些值反映了各个不同特征的表征“程度”（degree）。一个典型的RBF特征$i$会有一个高斯(钟形)响应函数$ \phi_i(s)$其取值仅受状态$s$和特征原型（中心状态）$c_i$间的距离及特征宽度$\sigma_i$的影响：
$$
\phi_i\doteq exp\left(-{||s-c_i||^2\over2\sigma^2_i}\right).
$$
可以根据当前状态或任务来选择看上去最合适的范数或距离标尺。图9.13展示了一个使用了欧几里德标尺的一维特征示例。

​	相较于二值特征，$\rm RBFs$的一个主要优势在于其拟合函数表现得非常光滑且可微。 虽然这很吸引人，但这在很多情况中却并没有实际意义。尽管如此，诸如在瓦片编码中应用$\rm RBFs$等分级响应函数（ graded response functions）的情况也已被（An, 1991; Miller et al., 1991; An, Miller and Parks, 1991; Lane, Handelman and Gelfand, 1992）等人广泛地研究了。

![](img\Figure 9.13.png)

**图 9.13：一维径向基函数**



所有这些方法无一例外地需要（比瓦片编码）更多的计算复杂度。也因此在状态空间大于二维时，往往会降低性能。在高维空间，瓦片的边缘（the edges of tiles ）会变得更加重要。另外，已有相关证明表示使用径向基无法在边缘附近有效地控制分级瓦片激活器（graded tile activations）。

​	$RBF$网络（network）是个线性函数拟合器，其使用$\rm RBFs$为特征。学习过程和其他线性函数拟合器完全一样，参考式（9.7）和（9.8）。另外，一些作用于$\rm RBF$网络的方法通过改变特征的中心值和宽度，将其变成非线性函数拟合。非线性方法可更精确地拟合目标函数。但$\rm RBF$网络，尤其是非线性$\rm RBF$网络的缺点在于，其计算复杂度会变得很高。在学习能保证稳定高效之前，往往需要大量的手动调参。



### 9.6 非线性函数拟合：人工神经网络

 	人工神经网络（$\rm ANNs$）被广泛用于非线性函数拟合中。$\rm ANN$是一个有着多个内连单元的网络，这个所谓的单元有着类似于神经元的属性，而神经元又是构成神经系统的主要组成部分。$\rm ANN$的发展有着很长的历史。在包括强化学习等在内的机器学习领域中，基于最新进展的深层$\rm ANN$模型均有着不错的表现。在第16章我们介绍了些令人振奋的的强化学习示例，它们也使用$\rm ANN$进行函数拟合。

​	图9.14展示了一个通用前馈$\rm ANN$,前馈意味着在该网络中是无环的，即没有一个单元的输出是可以影响到该网络的输入值的。在该网络中有一个由两个输出单元组成的输出层，一个由四个个输入单元组成的输入层以及两个隐含层：既不是输出也不是输出的层。 网络中每个实值权重与每个连接（link）相对应。该权重类似于人脑神经网络（见15.1节）中的突触效能（ efficacy of a synaptic）。如果$\rm ANN$的连接中有至少一个环，则其是一个循环而不是前馈$\rm ANN$。尽管强化学习中两种网络均有应用，但这里我们主要研究更简单的前馈$\rm ANN$。

​	这些单元（图9.14中的圆）是典型的半线性单元。这意味着这些单元将输入给其的信号先进行加权求和，再将结果输出至一个非线性函数，也称为“激活函数“（ activation function），用以产生该单元的输出，也叫激活值（activation）。尽管有很多激活函数可供使用，如有的时候诸如$ f(x) = max(0,x)$这类非线性整流器（ rectifier nonlinearity）也会用到，但基本都是$\rm S$（sigmoid）形的函数，比如逻辑函数$ f(x) = 1/1 + e^ −x $。若使用阶跃函数，比如当$ x ≥ θ$时，$f(x) = 1$，否则为$0$。则其结果会随着界限值$\theta$变成一个二值单元。另外，不同的层使用不同的激活函数往往是有用的。

![Figure 9.14](img\Figure 9.14.png)

**图9.14: 一个通用的前馈神经网络。其有四个个输入单元，两个输出单元，以及两个隐含层。**



​	在一个前馈$\rm ANN$中，每个输出单元的激活器就是在网络中对输入单元求其激活模式的非线性函数。这些函数通过网络中的连接权重参数交织在一起。一个没有隐含层的$\rm ANN$只可能对输入-输出函数做出有限的拟合。然而带有隐含层的$\rm ANN$却可以在输入空间的某个不大的区域内，利用大量有限但足够多的$\rm S$型单元来以任意精度拟合任意的连续函数（Cybenko, 1989）。只要满足温和条件（ mild conditions）。该结论也适用于其他非线性激活函数。但必须保证非线性：因为若一个多层前馈$\rm ANN$中所有单元使用的是线性激活函数，则整个网络等价于一个没有隐含层的网络（因为线性函数的线性函数还是线性的）。（dbsx）

​	理论和实践均已证明，单层ANN除了其所具有的”广泛适用的拟合“属性外，从前诸多人工智能任务中所需的复杂函数的拟合工作也比以前容易多了----为了达到目标，确实可能需要---诸多的抽象。这种抽象由诸多低层次的抽象分级组成。而低层次的抽象又由有着许多隐含层的深度模型抽象而成。(详见Bengio, 2009) 深度ANN的后继层又计算出了对于网络的”原始“输入更抽象的表述。其中每个单元代表了一个特征，作为整个网络的输入-输出函数中的的分级表述。

​	在人工智能领域，如何在不依赖于大量的手工特征的情况下，创造出上述的分级表述是个持久的挑战。这也说明了为何使用含隐含层的ANN学习算法在近些年获得了大量关注。一个典型的ANN是通过随机梯度法来学习的（图9.3）。每个权重通过往某个方向调整一点点来提升整个网络的性能。该性能由一个要求结果最大化或者最小化的目标函数来衡量。在普遍的监督学习中，给定一系列标签训练样本后，所谓的目标函数是期望误差或损失。而在强化学习中，可以将TD误差用于ANN中来学习估值函数，或者用于梯度bandit（2.7节）、策略梯度中来最大化期望回报。在所有这些案例中，如何评估在那些交织在一起的权重中，单个权重的改变对整个网络性能的影响是很有必要的。换言之，在给定当前网络的所有权重值的情况下，针对每个权重，即对于目标函数的偏导，评估其对整个网络性能的影响。所谓的梯度就是由这些偏导组成的向量。

​	作用于含隐含层的ANN的最成功的学习方法是反向传播算法，其通过贯穿于整个网络的前馈和反馈过程来执行。在前馈过程中，给定当前网络的输入单元的激活值后，会计算一遍所有单元的激活值。在每一轮前馈过程后，反馈中则高效地计算了每个权重的偏导。（就像在另一个随机梯度算法中一样，这些偏导组成的向量是对真实梯度的估计值。）在第15.10节我们讨论了一个利用强化学习原则而不是反向传播的方法来训练含隐含层的ANN。相比于反向转播，这些方法有些低效。但其更接近于真实的神经网络工作机制。

​	在仅有1、2层的浅层的神经网络中使用反向传播算法可以取得不错的结果，但在更深层次ANN中却不怎么好。事实上，一个有着$k+1$ 隐含层的ANN训练结果是可能会比含$k$个隐含层的ANN的训练的训练结果差的。即便深层的ANN能表示所有浅层ANN的函数，其结果也同样如此 (Bengio, 2009)。要解释这个现象可不容易，但确有几个关键因素。首先，一个有着大量权重的典型ANN将很难规避过拟合的问题。即，无法对未经训练的样本给出足够正确的估计。其二，反向传播之所以不适用于深度ANN是因为经反向传播计算而来的偏导会朝着网络输入方向快速衰退而导致学习过慢，或者是快速上升而导致学习过于不稳定。如何处理这些问题将极大地影响深度ANN所带来的很多令人瞩目的结果。

​	在有限的训练数据中通过消耗自由度来调整函数时，会遇到过拟合问题。虽然在在线强化学习这种样本数量并不受限的任务上不是很成问题，但要做到有效的泛化依然是个不可忽视的要点。在ANN中普遍存在过拟合的问题。这个问题在深度ANN中尤其严重因为其中有大量的权重需要调整。现在已有很多方法被研究出来用以对抗过拟合问题。比如可以在验证数据集变现开始劣于训练集时停止训练（交叉验证）；也可以通过调整目标函数来惩罚过于复杂的拟合行为（正则化）；另外也可以引入权重间的依赖性来江都自由度（比如权重共享）。

​	一个可以显著降低过拟合影响的方法是由 Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov (2014) 提出的剔除法（dropout method）。在训练中，有些单元连同其连接被随机地剔除出网络。可以视其为在训练一个”更瘦“的网络。在测试的时候将经由更瘦的网络所训练出的结果混合进来将有助于提高泛化的性能。剔除法通过将每个输出的单元权重乘以该单元在训练中被保留下来的概率来高效地模拟整合过程。Srivastava et al.发现该方法显著地改进了（网络）泛化的性能。它允许单个隐藏单元学习出可以和其他随机选择的特征良好协作的特征。如此就增加了隐藏单元特征的多样性，从而使得网络并不会因为一些极少出现的情况而被训练得过于特殊化。

​	在解决深度ANN中的过拟合问题上Hinton, Osindero, and Teh (2006)取得了重大进步。它们所使用的用以训练的深度置信网络结构和这里讨论的深度ANN很类似。在他们的方法中，最深层的权重通过使用非监督学习算法训练，每次只训练一个参数？。不依赖于总体目标函数，无监督学习可以提取捕获输入流的统计规律的特征。首先最深层最先被训练，然后依赖于该层的输出，倒数第二深的层被训练，以此类推。这个过程一直持续到所有或者大部分层的权重值被调整至类似于可被用于监督学习的初始化值状态。然后该网络以目标函数为基准利用反向传播算法对值进行微调。相关研究表明这种方法一般比使用随机初试值的反向传播算法更加有效。要想通过使用该方法初始化权重来提高网络性能需要注意诸多因素。但一种思想认为（不管怎么说），该方法使得将网络置于一个权重空间，使得基于梯度的算法能表现得更好。

​	有一类叫做“深度卷积网络”（deep convolutional network）的深度ANN被证明在实际问题中有着非常成功的应用，包括瞩目的强化学习问题（第16章）。这类网络被专门用于处理一些诸如图片这类在空间数组中排列的高维数据。该网络的设计受到了大脑中早期视觉形成工作的启发 (LeCun, Bottou, Bengio and Haffner, 1998)。由于其特殊的构造，可直接使用反向传播来训练。而不用考虑之前描述的用于深度层的训练方法。

​	图9.15展示了深度卷积网络的架构。该实例由 LeCun et al. (1998)设计用以识别手写字符。其中包含调整过的卷积层和抽样层，在这些层后是一些全连接终层。每一个卷积层产生一系列特征映射。特征映射是个由单元数组组成的活动模式（a pattern of activity）。而每个单元在其感受野里对数据做着相同的操作，即通过上层（对于本例的第一层的卷积层，也可以是外部输入数据）来“看到”数据。特征映射的单元均是彼此独立的，但其感受野全都是一样的，不论是尺寸还是形状。其坐落于不同输入数据数组中的不同位置。在同一特征映射的单元共享权重。



图xxxx

图xxxx

这意味着特征映射会检测到同一个特征而无论其在输入数组中的何处。以图9.15中的网络为例，其第一个卷积层产生了$6$个特征映射，而每个由$28\times 28$个单元组成。而每个特征映射中的单元又有个$5 × 5$的感受野。这些感受野相互重叠（该例中是个4列5行的块）。结果就是，该$6$个特征映射中的每一个只有$25$个可供调整的权重。

​	深度卷积网络中的抽样层降低了特征映射中的空间分辨率。抽样层中的每个特征映射由诸多单元组成。而这些单元数值由上层卷积层的特征映射单元的感受野平均而来。以图9.15的网络为例，其第一个抽样层的$6$个特征映射的每个单元均是由第一个卷积层的某个特征的$2\times2$非叠加感受野平均而来。计算结果为$6$个$14\times14$的特征映射。该抽样层将网络的灵敏度降低至探测到的特征的局部空间。即，抽样层帮助网络对空间中的不变部分做出合适的反应。这样做是有意义的，因为一个在（图片）某区域被探测到的特征是可能在其他区域被复用的。

​	关于ANN的设计及训练优势，这里我们仅提到了部分适用于强化学习任务的内容。尽管当前强化学习理论主要局限于表格或者线性函数拟合法，但正因为ANN,尤其是深度ANN的非线性函数拟合法的加入，才有了一些瞩目的强化学习任务中的杰出表现。

​	

### 9.7 最小二乘TD法(Least-Squares TD)

​	在9.4节，我们通过线性函数拟合建立了TD(0)方法，其能保证渐进收敛。只要做到适当地减少补偿，直至TD固定点：
$$
θ_{TD} = A^ −1 b,
$$
其中有
$$
A\doteq \mathbb E[\phi_t (\phi_ t − γ\phi_{t+1} )^\sf T] 而 b\doteq \mathbb E[R_{t+1}\phi_t]
$$
我们也许会问，为何一定要迭代地计算该解决方案？这是在浪费数据！为何不通过计算$A$和$b$的估计值，再直接计算TD固定点这样来更有效率的利用数据呢?最小二乘TD(通常称为Least-Squares TD)就是基于这样的思想而诞生的。其估计式样是：
$$
\hat A_t\doteq\sum_{k=0}^t(\phi_k-\gamma\phi_{k+1})T+\varepsilon \rm I \quad 且\quad \hat b_t\doteq\sum_{k=0}^tR_{t+1}\phi_k \tag{9.19}
$$
（其中$\varepsilon\rm I$，对于一些较小的$\varepsilon>0$，能保证$\hat A_t$是可逆的）然后估计TD固定点：
$$
\theta_{t+1}\doteq\hat A_t^{-1}\hat b_t. \tag{9.20}
$$
这是线性TD（0）中最高效使用数据的一种算法。但其也有较昂贵的计算开销。记得半梯度TD(0)算法的内存和单步计算开销仅有$O(n)$。

​	那么LSTD有多复杂？根据上述公式，其复杂度似乎随着$t$的增加而增加。根据（9.19）的2个近似，使用之前所述的技巧（比如第2章）可以递增地计算LSTD，所以每一步（更新中）只需要固定时常。虽然如此，$\hat A_t$的更新离不开外积（列向量乘以行向量），所以这是一个矩阵更新；其所需的计算复杂度是$O(n^2)$，自然为了存储$\hat A_t$矩阵其所需的内存有$O(n^2)$。

​	一个更大的潜在问题是我们的最终计算步骤（9.20）需要用到$\hat A_t$的逆，而一般求逆的计算复杂度是$O(n^3)$。幸运的是，以外积之和这类特殊形式组成的矩阵，求逆仅需要$O(n^2)$的复杂度来完成递增更新：
$$
\begin{eqnarray}
\hat A_t^-1&=&(\hat A_{t-1}+\phi_t(\phi_t-\gamma\phi_{t+1})^\sf T)^{-1} \tag{来自（9.19）}\\
&=&\hat A_{t-1}^{-1}-{\hat A_{t-1}^{-1}\phi_t(-\gamma\phi_{t+1})^\sf T\hat A_{t-1}^{-1}\over1+(\phi_t-\gamma\phi_{t+1})^\sf T\hat A_{t-1}^{-1}\phi_t}, \tag{9.21}
\end{eqnarray}
$$
其中$\hat A_{t-1}\doteq\varepsilon\rm I$。虽然被称为”谢尔曼 - 莫里森公式“（ Sherman-Morrison

formula）的式（9.21）表面上看起来很复杂，

图xxx

图xxx

但其实际上仅包含向量-矩阵和向量-向量的乘法，算法复杂度实际为$O(n^2)$。因此我们可以保存矩阵$\widehat{ A^{-1}}_t$的逆，然后在式（9.20）中使用它。完成这些只需要$O(n^2)$的内存和单步计算。在下页（译者：原著中也不是下页，而是上页）中给出了完成的算法。

​	当然，$O(n^2)$的开销依然远大于半梯度TD法中的$O(n)$开销。为了达到LSTD中的高效数据利用率，是否值得承受如此大的计算开销呢？这取决于$n$的大小，学习速度的重要性以及系统其他部分的开销。关于LSTD，其不需要步长参数优势是有提及，但该特点的优势却可能被过度夸大了。以为其虽不需要步长参数，却仍然需要$\varepsilon$；如果选取的$\varepsilon$很小，则逆序列（ the sequence of inverses）的波动幅度会很大，反之若选择太大，则学习速率会太慢。另外，因为LSTD没有步长参数，所以其永不忘记（数据）。有时候这是我们所需要的，但当在强化学习和GPI任务中需要策略$\pi$改变时，该特点会变得有些麻烦。在控制应用中，典型的做法是将LSTD和其他机制结合在一起来诱使其健忘，进而消除因不需要参数而来的任何初始优势。



### 9.8 总结

​	为了适用于人工智能或大型工程应用，强化学习系统必须有足够的“泛化”能力。为了做到这点，在“监督学习”中任何已存在的“函数拟合方法”均可被使用。只要简单地将备份（backup）用作训练样本即可。

​	也许最适用于强化学习的监督学习方法是“含参数的函数拟合法”。其中策略被参数化为权重向量$\theta$。尽管权重向量中已有足够多的元素，其表示的状态空间依然很大，我们必须寻找一个拟合的解决方案。针对权重向量$\theta$，在“在策略”分布$d$的条件下，我们定义$\rm MSVE(\theta)$作为衡量$v_{\pi\theta}(s)$的误差函数。在在策略的案例中，$\rm MSVE$清晰地给了我们一个衡量不同函数拟合的标准。

​	为了找到好的权重向量，最流行的方法是“随机梯度下降法”(SGD)的变种。本章我们聚焦于使用了“固定策略”（fixed policy）的“在策略”，也称为策略更新或预测；适用于其的一个自然（学习算法）选择是“n步半梯度TD法”（ n-step semi-gradient TD），其中当$n=\infty$或$n=1$时，引出了2种特殊算法，梯度MC和半梯度TD(0).半梯度TD法不是真正的梯度法。因为在这类bootstrapping法（包括DP）中，虽然权重向量有在更新目标中出现，但在计算梯度时却没有被考虑---因此这类算法被称为“半”梯度算法。所以这类算法的结果不能参考经典的SGD。

​	然而，使用了“线性”函数拟合的半梯度法的结果还是不错的。在线性函数拟合中，被估值等于权重和特征间乘积的和。在理论上，基于线性的案例是最好理解的。其实际表现也不错，只要有合适的特征。为了将先验知识加入强化学习系统，特征选取是最重要的途径之一。这些特征可以是多项式形式的，过去其在在线学习的模式下泛化得并不好，尤其是强化学习中。更好的是根据傅立叶基选择特征，或根据具有稀疏重叠感受野的某种形式的粗编码。瓦片编码是粗编码中的一种，其计算效率尤其高而且灵活。径向基在一二维的任务中很有用，这种任务中其平滑变化的响应是重要的。LSTD是最高效的数据线性TD预测方法，但其所需的计算资源正比于权重数量的平方。然而其他方法的复杂度（仅仅）正比于权重数量。在非线性方法中，有基于反向转播和SGD变种来训练的人工神经网络；近年来名为“深度强化学习”（deep reinforcement learning）的这些方法变得非常热门。

​	对所有n并基于最佳误差界限内的MSVE，线性半梯度n步TD法保证在标准条件下收敛。 这个界限是对较高的n总是更紧密，并且当$n→∞$时接近零。然而，实际情况下学习会非常慢，所以一定程度（$1 < n < ∞$）的bootstrapping往往更受青睐。

​	

### 参考书目和历史备注

​	泛化和函数拟合一直是作为一个整体出现在强化学习领域的。Bertsekas and Tsitsiklis (1996), Bertsekas (2012), and Sugiyama et al. (2013)提出了应用于强化学习的一些关于函数拟合的先进方法。在本节最后会讨论强化学习中关于函数拟合的一些早期工作。



**9.3**	 在监督学习中用于最小化均方误差的梯度下降法是众所周知的。 Widrow and Hoff (1960)引入了最小均方算法（LMS），作为递增梯度下降法的原型。有许多文献可参考到与此相关的内容（比如：Widrow and Stearns, 1985; Bishop, 1995; Duda and Hart,1973）。

​	半梯度TD(0)法第一次被Sutton (1984, 1988)所发现。其也是线性TD(XXX)算法中的一员。关于TD(XXX)将在12章讨论。用“半梯度”来描述这些bootstrapping法是本书第二版新引入的。

​	最早在强化学习中使用状态整合的可能是Michie and Chambers’s BOXES system (1968)。关于在强化学习中使用状态整合的理论研究已由 Singh, Jaakkola, and Jordan (1995) and Tsitsiklis and Van Roy (1996)所展开。在状态聚合被刚开发的时候，其已被用于动态规划 (比如： Bellman, 1957a)。



**9.4**	Sutton (1988) 证明了在TD(0)中使用平均、最小MSVE来衡量的话，是可以保证收敛概率为1的。其中特征向量xxx是线性独立的。同一时间其他学者 (Peng, 1993; Dayan and Sejnowski, 1994;Tsitsiklis, 1994; Gurvits, Lin, and Hanson, 1994). 也证明了该收敛结论。另外，Jaakkola,Jordan和Singh (1994)也证明了在在线更新的情况下该结论同样成立。所有这些结果是基于独立的线性特征向量，其暗示了有多少状态就至少有多少权重元素xxx。关于更重要的案例，即包含一般（不独立的）特征向量的收敛问题研究由 Dayan (1992)首次提出。关于Dayan的研究成果的一个重要拓展和强化是由 Tsitsiklis and Van Roy (1997)证明的。他们证明了本节所述的主要结论，即线性bootstrapping法中渐近误差的界限。



**9.5**	我们所介绍的关于线性函数拟合的可能性范围是基于Barto (1990)相关内容的研究。



**9.5.3** 术语“粗编码”（coarse coding）源自于 Hinton (1984)，而图9.6是他的其中一幅作图。在强化学习系统中使用该类型的函数拟合的一个早期例子是由Waltz and Fu (1965)提供的。



**9.5.4**  Albus (1971, 1981)提出了瓦片编码，包括哈希（hashing）。他称其为“基于小脑模型的咬合控制器”（cerebellar model articulator controller），或者在有些文献中也被称为CMAC。虽然 Watkins (1989)已经使用“瓦片编码”来表述CMAC了，但该术语也是第一次出现在本书的第一版中。瓦片编码已被使用于诸多强化学习系统（比如：Shewchuk and Dean, 1990;Lin和Kim, 1991; Miller, Scalera和Kim, 1994; Sofge和White, 1992;Tham, 1994; Sutton, 1996; Watkins, 1989）和其他类型的学习控制系统（比如： Kraft和Campagna, 1990; Kraft, Miller和Dietz,1992）。本节介绍的内容主要来自Miller and Glanz (1996)的研究。



**9.5.5** 自从 Broomhead and Lowe(1988)将径向基函数（RBFs）和神经网络结合来做函数拟合后，该方法就受到了广泛关注。 Powell (1987) 评估了早期RBFs的使用，而Poggio and Girosi(1989, 1990)则广泛发展并应用了这种方法。



**9.6**	自McCulloch and Pitts (1943)提出将阈值逻辑引入作为抽象模型神经元的单元后，就标志着人工神经网络（$\rm ANNs$）的开始了。作为分类和回归的学习方法，$\rm ANNs$已经历了几个不同的阶段：大致来说，首先是单层$\rm ANNs$的感知阶段 (Rosenblatt, 1962)和ADALINE (ADAptive LINear Element (Widrow and Hoff,1960) ；使用了多层$\rm ANNs$来学习的误差-反向传播阶段(Werbos, 1974; LeCun, 1985; Parker, 1985; Rumelhart, Hinton,Williams,
1986)；以及当前着重于学习展示的深度学习阶段 (比如：Bengio, Courville和Vincent, 2012; Goodfellow, Bengio和Courville, 2016)。关于神经网络的很多书均来自于Haykin (1994), Bishop (1995)和Ripley (2007)。

​	将$\rm ANNs$用作强化学习中的函数拟合，这种做法可以追溯到Farley and Clark (1954)建立的神经网络。他将用来表示策略的线性阈值函数的权重交给类强化学习（ reinforcement-like learning）。 Widrow, Gupta, and Maitra (1973) 展示了一个类神经元的线性阈值单元来进行学习。他们称其为“通过使用评价器或可选bootstrap适应来学习”（ learning with a
critic or selective bootstrap adaptation），即ADALINE算法的强化学习版本。 Werbos (1974, 1987, 1994)开发了一种预测控制方法，其通过$\rm ANNs$的误差反向传播来学习策略，并结合类TD算法学习估值函数。 Barto, Sutton, 和Brouwer (1981) 、Barto和Sutton (1981b)将联合内存网络( associative memory network)(比如:Kohonen, 1977; Anderson, Silverstein, Ritz,
和Jones, 1977)扩展至强化学习任务中。 Barto, Anderson和Sutton(1982)使用一个两层ANN来学习非线性控制策略，并强调了第一层用于学习一个合适的表征（representation）的作用。关于使用多层$\rm ANNs$来学习估值函数，Hampson(1983, 1989)是其早期的支持者。Barto, Sutton和Anderson (1983)提出了基于ANN学习 木条平衡任务（详见15.7和15.8）的 执行-评价算法（ actor-critic algorithm）。Barto和Anandan (1985) 提出了基于Widrow, Gupta, and Maitra’s (1973) 的可选bootstrap算法的一个随即版本，称之为“联合奖励-惩罚算法”（associative reward-penalty (A R−P ) algorithm）。 Barto (1985, 1986)、Barto和Jordan (1987) 设计了一个经由全局扩散强化信号训练而来的xxx单元组成的$\rm ANNs$，将其用于学习非线性可分的分类规则。那个时期Barto (1985)研究了如何该方法运用于$\rm ANNs$并在文献中探讨了该学习规则和其他学习规则间的关系。(详见15.10节关于使用该方法来训练多层$\rm ANNs$的内容。) Anderson (1986, 1987, 1989) 评估了许多用于训练多层$\rm ANNs$的方法，并证明了在平衡木和汉诺塔任务中，若使用2层$\rm ANNs$，借助于执行-评价算法（执行器和评价器同时作用于$\rm ANNs$），并通过误差反向传播训练出来的结果要远远好于单层$\rm ANNs$。Williams (1988)设计了一些方法将反向传播和强化学习同时整合进$\rm ANNs$的训练。Gullapalli (1990) and Williams (1992) 设计了专用于有着连续而非二值输出的类神经元强化学习算法。 Barto, Sutton, and Watkins (1990)认为在序列决策问题中，$\rm ANNs$可作为一个重要组成部分来完成函数拟合。 Williams (1992) 将“加强”（REINFORCE）学习规则（详见13.3节）加入到误差反向传播法中来训练多层$\rm ANNs$。 Schmidhuber (2015) 审查了用于强化学习中的$\rm ANNs$，其中包括循环$\rm ANNs$。



**9.7**	LSTD最先由 Bradtke and Barto (参考：Bradtke, 1993, 1994; Bradtke and Barto, 1996; 			Bradtke, Ydstie, and Barto, 1994)提出，并由 Boyan (2002)进行了深入地研究。至少在1949	年，逆矩阵的增量更新已被人们所熟知 (Sherman and Morrison, 1949)。	



​	关于使用函数拟合来学习估值函数，（我们已知）最早使用该技术的案例是“塞缪尔的跳棋选手”（ Samuel’s checkers player）(1959, 1967)。塞缪尔听取了 Shannon (1950)的建议，认为在游戏中未必需要将估值函数设计导向，以供选择有用的操作（moves），而且估值函数可以借助特征来进行线性函数拟合。在该游戏试验中，塞缪尔不仅尝试了线性函数拟合，也尝试了查找表（ lookup tables）和称为签名表的分层查找表（ hierarchical lookup tables）（Griffith, 1966, 1974; Page, 1977; Biermann, Fairfield, and Beres, 1982）。

​	在几乎塞缪尔开展相关工作的同一时间，Bellman和Dreyfus (1959) 提出了将函数拟合用于DP。（我们不禁好奇是否 Bellman和塞缪尔在彼此工作中有着相互促进作用，但并没有相关证据。）现在，关于函数拟合以及DP已经有相当数量的参考文献了，比如多网格法和使用样条和正交多项式的方法 (e.g., Bellman and Dreyfus, 1959; Bellman,Kalaba, and Kotkin, 1973; Daniel, 1976; Whitt, 1978; Reetz, 1977; Schweitzer and Seidmann, 1985; Chow and Tsitsiklis, 1991; Kushner and Dupuis, 1992; Rust, 1996)。

​	Holland(1986)的分类系统运用了可选特征匹配技术，来评估所有状态-动作对。每个分类器用以匹配一个特定的状态子集，该子集的部分特征有特定的值，而其他的特征值则取任意值（犹如扑克中的百搭牌“wild cards”）。然后在典型的状态聚合法中使用这些子集来进行函数拟合。Holland的想法是将遗传算法运用于一组分类器，总体来说可以被当作一个有用的动作值函数。Holland的想法影响到了早期研究强化学习的作者，但我们这里聚焦于其他不同的函数拟合方法。因为将分类器用于函数拟合是有诸多局限的。首先，其属于状态整合法，在缩放（scaling）和有效展示光滑的函数时其有着（抹之不去的）影响。另外，分类器的匹配规则仅和聚合边界有关，而该边界和特征轴是没有关系的。传统的分类器系统最主要的局限可能在于其分类器是从遗传算法（一种调优法）学习而来的。正如第1章所述，通过学习而不是调优法，agent能发现更多关于如何学习的细节信息。这个观点指引我们转而使用监督学习，尤其是梯度下降和神经网络来完成强化学习任务。Holland和我们采用的方法有着如此般的不同并不是件出人意料的事。因为Holland提出其方法的时期正好是神经网络被普遍认为因计算能力太差而表现太弱的时候。而我们开展相关工作时，正好是人们刚开始对该传统方法提出广泛质疑的时期。（应该说）依然有很多的机会将这2个不同的方法整合到一起。

​	Christensen和Korf（1986）将回归法用于国际象棋中以对其线性估值函数的系数进行修正。 之后Chapman和 Kaelbling (1991) 和Tan (1991) 采用了决策树来对估值函数进行学习。目前基于解释的学习方法也已适应学习价值函数，其能够产出足够紧凑的表征(Yee, Saxena, Utgoff和Barto, 1990; Dietterich and Flann, 1995)。
