# 策略梯度方法

在这一章当中，我们来思考一些新的方法。在这一本书当中，到目前为止的所有的方法都是通过学习到各种动作的值函数，然后基于各个动作的值函数估计值来选择一个动作；如果没有估计的动作-值函数，甚至该策略都不会存在。在这一章当中，我们来思考一些不需要计算值函数的方法--一些通过参数化策略的方法。当然，在计算策略的权重的时候可能还是会用到值函数，但是在动作的选择当中则完全不需要值函数。我们仍然使用 $\theta \in {R}^n$ 表示主要的权重向量，但是在这一章当中表示的是策略的权重向量。因此，我们用  $\pi (a|s,\theta)=Pr\lbrace A_{t}=a|S_{t}=s,\theta_{t}=\theta\rbrace$来表示时刻$t$ 在状态$s$ 下面基于当前的参数 $\theta $ 采取动作$a$ 的概率。如果在某一种方法当中同时也使用到了值函数，那么值函数当中的参数为了区别于$\theta$ 使用$w$ 来表示，即 $\hat{V}(s,w)$ 。

在这一章当中，我们讨论的是基于一些和策略权重相关的性能函数 $\eta(\theta)$ 的梯度来学习策略的权重。这些方法的目标都是为了最大化性能函数，因此通过梯度上升法来更新估计性能函数$\eta(\theta)$ 当中的$\theta$ ：
$$
\theta_{t+1}\dot{=}\theta_{t}+\alpha\hat{\nabla \eta(\theta_{t})}
$$
其中的$\hat{ \nabla \eta(\theta_{t}) }$ 是性能函数关于$\theta_{t}$ 的梯度随机的期望估计。不管有没有使用值函数，我们都将所有基于这种框架的方法称作是策略梯度（Policy Gradient）方法。通常，同时学习策略以及值函数的估计的方法被称作是 执行－评价（actor-critic) 方法，其中的actor指代学习到的策略，critic代表的是学习到的值函数，通常来讲是一个状态值函数。首先，我们来看某一个时刻的事例，其性能通过从某一起始状态开始，在参数化的策略下的值函数来衡量：$\eta(\theta)=v_{\pi\theta}(s_{0})$ 。再考虑接下来的事例之前，性能评价通过平均的奖励率来定义：$\eta(\theta)=r(\theta)$ 。这样在后面我们能够通过非常简单的方式为两种情况来表现算法。



## 13.1 策略估计和它的优点

在策略梯度当中，策略可以用任意的方式进行参数化，只要$\pi (a|s,\theta)$ 关于权重可导进行，也就是说 $\nabla_{\theta}\pi (a|s,\theta)$ 存在并且有限就可以。在实际情况中，为了确保探索我们一般还会要求策略绝对不要变为确定性的，也就说：$\forall s,a,\theta: \pi (a|s,\theta)\in(0,1)$ 开区间。在这一小节当中，我们将介绍离散的动作空间当中最常用的参数化方式，并且指出它对于一般的动作值函数方法的优势。实际上基于策略的方法一是在连续的动作空间当中有效的方法，我们将在后面的13.7小节当中介绍。

如果动作空间是离散的并且并不是很大，那么一种很自然的参数化的方式就是对每一个状态动作对都单独的构造一个表达：$h(a,s,\theta)\in R$ 。那么对一个状态其中最偏好的动作就给予被选中的最高的概率，如通过一个指数的softmax进行表示：
$$
\pi(a|s,\theta)\dot{=}\cfrac{exp(h(s,a,\theta))}{\Sigma_{b}exp(h(s,b,\theta))}
$$
这里的$exp(x)=e^{x}$ ,其中的e=2.71828表示的是基本的自然对数的底数。 注意这里的分母只是为了让各个状态下的所有动作的概率加起来为1。但是他们各自的表示还能够被确定的参数化。例如，它们能够被深度神经网络计算，这里的$\theta$ 就是网络当中所有相连的权重，就像我们在章节16.6当中描述的深度Q网络（Deep-Q-network）一样。或者说这个表达式能够简单的通过特征当中的线性组合来表达：
$$
h(s,a,\theta)\doteq\theta^{\top}\phi(s,a)
$$
其中用到的特征向量可以通过第9章当中的任意方法来构建。



通过动作值函数采用$\xi-greedy$ 的方法选择动作始终会以$\xi$的概率进行随机的动作选择。因此，通过动作公式（2）当中的softmax 进行动作的选择有一个直接的优点，就是对策略的估计能够得到一个确定性的动作。当然，动作值函数的方法也可以使用softmax进行动作选择，但是单独的使用仍然不能得到一个确定性的动作。相反，动作值函数的估计能够收敛于一个它的真实值，这个值存在一定的差异，并且也能够影射到0到1之间的一个具体的概率。如果softmax当中包含一个热度参数，那么随着时间的推移这个热度会降低以至于接近于一个确定的理论。但是在实际情况中，制定降低的时间规划是非常困难的，甚至对于热度参数的初始化都非常的困难。在不知道关于真实的动作值函数的情况下，我们更愿意进行假设。偏好的动作会很不同，因为其并没有接近于某种具体的值；但是这些动作却倾向于产生最优的随机策略。如果最优策略是存在的，那么对于最优动作的偏好会一定程度的高于次优的策略（在参数允许的情况下）。

也许相比于对动作-值函数的参数化，对策略进行参数化最简单的优势就是策略可能是一个更简单的估计函数。根据问题的不同，策略函数和动作-值函数的复杂程度也不同。对于有一些问题，动作-值函数要简单一些，并且也容易估计；对于另外一些问题，策略的情况更加简单一些。在后者的这种情况当中，基于策略的方法将会明显的学习的更快并且达到一个无限接近于监督学习的策略。

在具有重要的函数估计的问题当中，最好的估计策略就是随机。例如，在具有不完备信息的牌类游戏当中，最优的策略通常都是以一定的概率做两件事情，比如说在扑克当中何时进行虚张声势。使用动作-值函数来找到一个随机的最优策略显得很不自然，但是正如例子13.1当中所示，策略估计的方式确十分的合适。这是基于策略方法的第三个优势。

例13.1

考虑如图13.1所示的一个小的格子世界。正常的每一步策略都是－1。在三个非终止状态中，每一个状态都只有两种动作：左移或者是右移。在第一和第三个状态当中，动作具有正常的结果；但是在第二个状态当中动作会被反转，也就是说向右移动的动作会左移，向左移动的动作会变成右移。这个问题很困难，因为在函数估计当中各个状态将会变得相同。具体而言，对于所有的状态s，我们定义$\phi(s,right)=[1,0]^{T}$ 以及 $\phi(s,left)=[1,0]^{T}$。在动作-值函数当中通过$\xi-greedy$选择动作的方式会迫使策略只在两种策略当中选择：在所有的步骤当中以高概率1-$\xi/2$选择右移的动作，或者是以同样的高概率选择左移的动作。这两种策略（起始的状态的值）分别是X和Y。如果某种方法能够学到选择左移或者是右移的概率，那么这种方法显然是更好的。图13.1展示了在所有的状态下把开始状态的值作为函数来选择右移的概率情况。其中最好的概率分布当中，2/3以上的情况都是通过策略梯度的方式求解得到的。



图13.1 ：随机策略下的开始状态的值的情况，随机策略在所有的状态当中，采取同样的概率选择左移或者是右移。采用$\xi-greedy$的方法，令其中的$\xi=0.1$得到的效果仅仅是图中 的X和Y，但是采用策略梯度得到的最好的随机策略却可以达到Z。

最后，我们发现选择对策略进行参数化会是一种非常好的将期望的策略形式的先验知识注入到强化学习系统当中的方式。



## 13.2 策略梯度理论

除了之前的章节当中介绍的参数化策略在应用当中的实际好处之外，参数化策略还有很多理论推导中的好处。。。。（作者：实际上我不知道要怎么写这里，这里要依赖于地10章的内容）。这里喔必须要要定义一个性能度量函数$\eta$。这里存在两种定义：一种定义于片段型（episodic）情形；而另一种定义于连续性的情形。我们尽量的去阐述这当中的所有事情，以至于这两种情况能够使用一套符号以及描述。

但是，我们鼓励读者首先来考虑片段型情形，在这种情形当中，我们将性能函数定义为片段当中初始状态的值函数。我们可以在不损失掉任何有意义的一般性原则下作出这样的假设：每一个片段情况都是以一些具体的状态$s_{0}$开始。于是，我们可以定义性能函数为：
$$
\eta(\theta)\doteq v_{\pi_{\theta}(s_{0})}
$$
这里的$v_{\pi_{\theta}}$表示的是$\pi_{\theta}$的真实值函数，策略由策略权重参数$\theta$决定。

接下来，我们将讨论在有了函数估计的情况下，如何解决通过改变策略参数的方式来确保提高值函数。待解决的问题同时依赖于动作的选择以及所处的选择动作的状态，并且者二者都被策略权重参数所影响。给定一个状态，策略权重参数对于动作以及相应的奖励的影响可以以相对直接的方式从策略的参数化当中计算出来。但是策略对于状态分布情况的影响完全是一个关于环境的函数，并且一般来讲都是未知的。那么，性能梯度依赖于状态分布，我们在当改变策略对于状态分布的影响未知的情况下，应该如何通过策略参数进行性能梯度的估计？

这就让我们引出策略梯度理论，他给我们提出了一个性能梯度关于策略参数的代数表达式（那就是说，我们需要去估计公式（1）当中的梯度下降），这个和状态分布的微分密切相关。策略梯度的理论是：
$$
\nabla \eta(\theta)=\sum_{s}d_{\pi}(s)\sum_{a}q_{\pi}(s,a)\nabla_{\theta}\pi(a|s,\theta)
$$
这里，在所有的情形当中，梯度都是一个关于$\theta$当中各个成分的偏导的一个列向量，其中的$\pi$说明策略和权重向量$\theta$相关。这里的分布的表达式$d_{\pi}$和第9章第10章当中的描述相同(其实就是状态的先验概率)。也就是说，在一个片段的情形当中，$d_{\pi}(s)$定义为在t次状态当中，采样的结果当中$S_{t}=s$的期望的次数，而采样采取的是从状态$s_{0}$开始，在动态MDP当中采用策略$\pi$进行随机采样得到的状态。在下面是对策略梯度理论的证明：

证明：片段情形当中的策略梯度理论







# 13.3 REINFORECE: 蒙克卡洛策略梯度

目前，我们已经做好了第一个策略梯度算法的准备工作。回顾随即梯度下降法的思想，对于它来讲我们需要得到一些值函数的样本，这些样本的期望要等价于性能的梯度。在策略梯度理论的公式（5）当中，给了我们一个准备的梯度的表达式，我们需要做的就是采样这些期望，让它们等于或者是约等于策略梯度表达式。注意到，等式的右边是一个状态的值函数的加权和，权重表示的是这个状态在策略$\pi$的情况下呗访问到的频率，同时还根据到达这个状态的时间被一个参数$\gamma$加权。如果我们仅仅是根据策略，对于某个状态我们就会以一个固定的比例遇到它，但是假如了一个$\gamma^{t}$就会防止出现这个期望值。因此：

$\nabla \eta(\theta)=\sum_{s}d_{\pi}(s)\sum_{a}q_{\pi}(s,a)\nabla_{\theta}\pi(a|s,\theta)$

$\nabla \eta(\theta)=E_{\pi}[\gamma^{t}\sum_{a}q_{\pi}(S_{t},a)\nabla_{\theta}\pi(a|S_{t},\theta)].$

这是非常重要的一步，我们还将继续的拓展，对于动作也采取同样的方式进行处理（用$A_{t}$替代公式当中的$a$）。上式期望当中还保留的是对动作的累加；要是每一项只是动作选取的概率就好了，也就是说根据$\pi(a|S_{t},\theta)$。让我们来构造这样的方式吧，乘以然后除以这一个概率，接着上面的等式，我们得出

$\nabla \eta(\theta)=E_{\pi}[\gamma^{t}\sum_{a}\pi(a|S_{t},\theta)q_{\pi}(S_{t},a)\frac{\nabla_{\theta}\pi(a|S_{t},\theta)}{\pi(a|S_{t},\theta)}],$ 

$=E_{\pi}[\gamma^{t}q_{\pi}(S_{t},A_{t})\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}],$ (将a用样本$A_{t}～\pi$代替)

$=E_{\pi}[\gamma^{t}G_{t}\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}].$(其中$E_{\pi}[G_{t}|S_{t},A_{t}]=q_{\pi}(S_{t},A_{t})$ )



这就是我们真正想要的东西，一个在每一个时间步骤都可以采样，并且其梯度等于期望的一个量。用这个简单的表达来表示公式（1）当中的一般性的随机梯度下降法，我们得到更新的公式：
$$
\theta_{t+1}\doteq\theta_{t}+\alpha \gamma^{t}G_{t}\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}
$$
我们将这种算法称作是REINFORCE算法（1992，Williams命名）。这种更新的理念非常的直观。每一个增量都正比于返回值$G_{t}$和一个向量的乘积，这个向量表示为一个将要采取的动作的梯度除以该动作被选中的概率。这个向量朝着权重空间当中最能够增加在未来要访问的状态$S_{t}$当中要重复的动作$A_{t}$的概率的方向。这个更新朝着正比于这个返回值，反比于动作被选中的概率大小。前者非常直观，它让权重朝着青睐的动作产生最大的回报的方向；后者也是作用明显，因为如果不除以这一项概率高的动作会具有经常被选中的优势，这样即便是得到最高的回报它们也会胜出（更新就会常常饿朝着这个方向）。

注意到REINFORCE算法使用的是从$t$时刻开始的完全的 反馈，它包括了到这个事件片段的结尾的所有的奖励。从这个层面来看，REINFORCE算法是一种蒙特卡洛算法，他被定义为一种当片段式事件完成之后在回过头来看各个更新的算法（像是第五章当中的蒙特卡洛算法）。下面的伪代码对其细节进行描述。

##### 算法13.1 REINFORCE，一种基于策略梯度的蒙特卡洛算法（片段式的）

输入：可微的策略参数化表达 $\pi(a|s,\theta),\forall a\in \mathscr A,s\in \mathscr S, \theta\in R^{n}$

初始化策略权重$\theta$

一直迭代：

​    根据策略$\pi(.|.,\theta)$生成一个采样片段：$S_{0},A_{0},R_{1},….,S_{T-1},R_{T}$，对于片段当中的每一步$t=0,1,....T-1:$

​           $G_{t}\leftarrow 从第t步的返回$

​           $\theta \leftarrow \theta+\alpha\gamma^{t}G_{t}\nabla_{\theta}log\pi(A_{t}|S_{t},\theta) $



REINFORCE算法更新当中的向量$\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}$ 仅仅将策略的参数化表达放在其中，这个向量在不同的文献当中有不同的符号表示和名称，我们将其简单的成为资格向量（eligibility vector）。资格向量通常被写作是一个简化后的表达式：$\nabla_{\theta}log\pi(A_{t}|S_{t},\theta) $，因为$\nabla log x=\frac{\nabla x}{x}$ 。本章当中其余的所有位置都使用这个表达式。这一章前面内容的例子当中，我们考虑了指数的softmax策略（2），其中采用线性的方式构成动作的表达（3），对于这个的参数化表达，它的资格向量是
$$
\nabla_{\theta}log\pi(a|s,\theta)=\phi(s,a)-\sum_{b}\pi(b|s,\theta)\phi(s,b)
$$
作为一种随机梯度算法， REINFORCE算法具有非常好的收敛性。从其构成来讲，在一个采样周期当中的更新方向适合性能梯度方向一致的。这就保证了对于充分小的$\alpha$有一个期望的性能的提高，并且在标准的随机估计的条件下降低$\alpha$能够收敛于一个局部最优解。但是，由于REINFORCE算法是一种蒙特卡洛算法，具有很高的方差，因此学习的速度非常的慢。

我们将该算法用于例子13.1和13.2当中的格子世界（grid worlds），来得到一个结果。。。



**练习13.1**  使用定义和初级微积分证明公式（13.7）



# 13.4 有基准（Bsseline）的REINFORCE算法

通过将公式（5）当中的动作值函数和一个基准（Baseline）$b(s)$进行比较可以将公式（5）当中的策略梯度理论一般化：
$$
\nabla \eta(\theta)=\sum_{s}d_{\pi}(s)\sum_{a}(q_{\pi}(s,a)-b(s))\nabla_{\theta}\pi(a|s,\theta)
$$
这个基准可以是任意函数，甚至是一个随机的变量，只要它不是随着动作a的不同而不同；这个等式还是成立的，因为减掉的这一项还是0:

$\sum_{a}b(s)\nabla_{\theta}\pi(a|s,\theta)=b(s)\nabla_{\theta}\sum\pi(a|s,\theta)=b(s)\nabla_{\theta}1=0  ,\forall s\in \mathscr S$

然而，当我们使用之前的策略将策略梯度转化为一个期望和一个更新的策略，这其中的基准就能够明显的将其中的方差降低。

因此，最终我们得到一个包含基准的新版REINFORCE算法：
$$
\theta_{t+1}\doteq\theta_{t}+\alpha \gamma^{t}(G_{t}-b(S_{t}))\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}
$$
由于公式当中的基准可以是一个0向量，因此这是REINFORCE算法更一般的表达。一般来讲，基准可以是的期望值函数不变，但是对于方差的影响却是非常的大。例如，我们在2.7节当中看到模拟基准可以明显的降低梯度类算法的方差，并且因此提高学习的速度。在这一类的算法当中，这种值仅仅是一个值（目前为止是讲理的平均值），但是对于MDPs基准应该根据状态s的不同而不同。在某些状态当中所有的动作都具有较高的值函数，于是我们就需要一个高的基准来让具有高值函数的动作从低的值函数的动作当中被区别出来；在另一些状态当中，所有的动作都具有低的值函数因此需要一个低的基准。

一种自然的基准的选择方式是对状态值$\hat{v}(S_{t},\mathbf w)$进行估计, 其中$\mathbf w\in \mathbb R^{m}$是使用前面章节方法学到的第二个权重向量。由于REINFORCE算法是一种通过蒙特卡洛算法来学习策略权重$\theta$的算法，很自然对于状态值函数权重$\mathbf w$的学习也可以采用蒙特卡洛算法。带基准的REINFORCE算法完整的伪代码如下，其中的基准也通过学习到的状态值函数来表示。

这里对于步长参数$\alpha$ 和$\beta$的讨论也是非常好的。值函数的步长的选择相对来说比较的简单，有经验规则可以参考；但是对于对于动作值函数的步长的选择就不是那么的清楚了。它依赖于奖励的方差以及策略的参数化表示。

##### 算法13.2 带基准的REINFORCE算法（片段式的）

输入：一个可微的策略参数化表示 $\pi(a|s,\theta),\forall a\in \mathscr A,s\in \mathscr S, \theta\in \mathbb R^{n}$

输入：一个可微的状态值函数参数化表达 $\hat{v}(s,\mathbf w),\forall s\in \mathscr S, \mathbf w\in \mathbb R^{m}$

参数：步长$\alpha>0$, $\beta>0$

初始化策略权重$\theta$和状态值函数权重$\mathbf w$

一直迭代：

​    根据策略$\pi(.|.,\theta)$生成一个采样片段：$S_{0},A_{0},R_{1},….,A_{T-1},S_{T-1},R_{T}$，对于片段当中的每一步$t=0,1,....T-1:$

​           $G_{t}\leftarrow 从第t步的返回$

​          $\delta\leftarrow G_{t}-\hat{v}(S_{t},\mathbf w)$

​          $\mathbf w\leftarrow \mathbf w+\beta\delta\nabla_{w}(S_{t},\mathbf w)$

​           $\theta \leftarrow \theta+\alpha\gamma^{t}G_{t}\nabla_{\theta}log\pi(A_{t}|S_{t},\theta) $



# 13.5 策略子－评判算法（Actor-Critic）

虽然，带基准的REINFORCE算法同时学习了策略以及状态值函数，但是我们仍然不将其看作是一种策略子－评价算法（以下表示为actor-critic算法），因为其中的状态值函数只用做基准，并没有进行评价（critic）。也就是说，基准并没有进行bootstrapping（也就是根据序列状态的值函数更新当前的状态），仅仅作为待更新状态的基准。这是一个非常有效的区分方式，因为只有迭代更新的方式才会引入一个基于估计函数的性能的偏执和一个渐近线。正如我们所看到的，迭代更新引入的偏执以及对状态表达的依赖通常都在降低方差以及加速学习效率当中起到一个平衡的作用。带基准的REINFORCE算法是一种无偏估计，并且还会渐近的收敛于一个最小值；但是和大多数的蒙特卡洛算法一样，高方差使得其学习的很慢。正如之前我们在这一本书当中看到的一样，使用时序差分（TD）算法可以消除这种不方便；通过多步（muti-step)的算法我们可以灵活的选择迭代的程度。为了在策略梯度算法当中使用到这些好处，我们采用一种具有真正的迭代评价的actor-critic算法。

首先，我们来考虑单步的actor-critic算法，就像在第6章当中提到的TD算法如TD（0），Sarsa（0）以及Q-learning算法。单步算法最主要的好处是他们是完全的在线和累积的，因此避免了资格轨迹的复杂性。它们是一种特殊的资格轨迹算法，并不是一种一般性的情况，但是非常便于理解。单步的actor-critic算法采用单步的返回替代了公式（9）当中的全部返回（并且采用一个学习到的状态值函数作为基准），算法伪代码如下：
$$
\theta_{t+1}\doteq\theta_{t}+\alpha (G_{t}^{1}-b(S_{t}))\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}
$$

$$
=\theta_{t}+\alpha (R_{t+1}+\gamma \hat{v}(S_{t+1},\mathbf w)-\hat{v}(S_{t},\mathbf w))\frac{\nabla_{\theta}\pi(A_{t}|S_{t},\theta)}{\pi(A_{t}|S_{t},\theta)}
$$



##### 算法13.3 带基准的REINFORCE算法（片段式的)

输入：一个可微的策略参数化表示 $\pi(a|s,\mathbf \theta),\forall a\in \mathscr A,s\in \mathscr S, \theta\in \mathbb R^{n}$

输入：一个可微的状态值函数参数化表达 $\hat{v}(s,\mathbf w),\forall s\in \mathscr S, \mathbf w\in \mathbb R^{m}$

参数：步长$\alpha>0$, $\beta>0$

初始化策略权重$\mathbf\theta$和状态值函数权重$\mathbf w$

一直迭代：

​    初始化S(第一个状态)

​    $I\leftarrow1$

​    当S不是终止状态的时候：

​    根据策略$A～\pi(.|S,\theta)$

​    采取一个动作$A$,得到一个观测$S^{'}, R$

​          $\delta\leftarrow R+\gamma\hat{v}(S^{'},\mathbf w)-\hat{v}(S,\mathbf w)$

​          $\mathbf w\leftarrow \mathbf w+\beta\delta\nabla_{w}(S,\mathbf w)$

​           $\theta \leftarrow \theta+\alpha\delta I\nabla_{\theta}log\pi(A|S,\theta) $

​          $I\leftarrow \gamma I$

​          $S\leftarrow S^{'}$



一般性的状态值函数学习算法等价于半梯度的TD（0）算法，算法的伪代码在上面。注意这里将是一个完全在线的并且不断累积的一个算法，随着状态，动作以及奖励的不断进行，他们出现之后就不会再一次的被访问。

因此，如果正向的思考这个问题，更具有一般性的多步算法以及接下来的$\gamma -$返回算法更加的直接，仅仅将公式（10）当中的$G_{t}^{(n)}$ $G_{t}^{\gamma}$分别替换就好了。反向思考这个问题的思路也是非常直接的，根据第12章当中的模式，对actor和critic使用单独的资格轨迹。下面是该算法的伪代码。

接下来我们会采用一些例子来说明它的好处。。。。。。。。。。。。







